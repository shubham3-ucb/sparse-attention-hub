@startuml
skinparam style strictuml

' === Sparse Attention ===
abstract SparseAttention {
  + custom_attention(): Tuple[Tensor, Optional[Tensor]]
  + pre_attention_hook_generator(): void
}
abstract EfficientAttention extends SparseAttention {
  + custom_attention(): Tuple[Tensor, Optional[Tensor]]
}
abstract ResearchAttention extends SparseAttention {
  + masks: Sequence(ResearchMasker)
  + custom_attention(): Tuple[Tensor, Optional[Tensor]]
}
class DoubleSparsity extends EfficientAttention {
  + custom_attention(): Tuple[Tensor, Optional[Tensor]]
}
class HashAttention extends EfficientAttention {
  + custom_attention(): Tuple[Tensor, Optional[Tensor]]
}

abstract ResearchMasker {
  + add_mask(keys, queries, values, previous_attention_mask, prev_num, prev_den, List[ResearchMasker])
  + get_attention_numerator(keys, queries, values, mask)
  + get_attention_denominator(keys, queries, values, mask)
}

abstract SamplingMasker extends ResearchMasker
abstract FixedMasker extends ResearchMasker

abstract topKMasker extends FixedMasker
abstract topPMasker extends FixedMasker
class RLocalMasker extends FixedMasker
class RCausalMasker extends FixedMasker
class RSinkMasker extends FixedMasker

class RPQCache extends topKMaster
class ROracletopK extends topKMasker
class RHashAttention extends topKMasker
class RDoubleSparsity extends topKMasker


class RRandomSampling extends SamplingMasker
class RMagicPig extends SamplingMakser


class SparseAttentionMetadata {
  + layer_wise_state: Dict
  + global_state_: Dict
}

' === Adapter System ===
class Request {
  + context: String
  + questions: Union[String, List[String]]
}

class RequestResponse {
  + responses: Union[String, List[String]]
}

abstract ModelHubAdapterInterface {
  + process_request(Request): RequestResponse
}

abstract SparseAttentionAdapterInterface {
  + get_custom_attention_function(SparseAttention): Callable
}

abstract ModelAdapter implements ModelHubAdapterInterface, SparseAttentionAdapterInterface {
  + model_name: String
  + sparse_attention_config: SparseAttentionConfig
  + sparse_attention: SparseAttention
  + enable_sparse_mode(): Generator
}

class ModelAdapterHF extends ModelAdapter {
  + model: PreTrainedModel
  + tokenizer: AutoTokenizer
  + device: String
  + process_request(Request): RequestResponse
  + enable_sparse_mode(): Generator
  + get_custom_attention_function(SparseAttention): Callable
}

' === Deprecated Classes (marked for removal) ===
note top of SparseAttentionGen : DEPRECATED\nReplaced by ModelAdapterHF
abstract SparseAttentionGen {
  + get_custom_attention_function()
  + __call__()
}
class SparseAttentionHF extends SparseAttentionGen {
}

note top of ModelHub : DEPRECATED\nReplaced by ModelAdapterHF
abstract ModelHub {
  + api_token: String
  + addPreAttentionHooks(model, hook_generator, hook_name)
  + removePreAttentionHooks(model, hook_name)
  + replaceAttentionInterface(model, attention_interface_callable, attention_interface_name)
  + revertAttentionInterface(model)
}
class ModelHubHF extends ModelHub{
  + field: type
  + method(type): type
}

note top of Pipeline : DEPRECATED\nRemoved in new architecture
abstract Pipeline {

}
class PipelineHF extends Pipeline{
 
}

note top of SparseAttentionServer : DEPRECATED\nRemoved in new architecture
class SparseAttentionServer {
  + port: String
  + execute(ModelHub, SparseAttentionStrategy)
}



' === Benchmark  ===

abstract Benchmark {
  + name
  + subsets
  + create_hugging_face_dataset()
  + run_benchmark()
  + calculate_metrics
}

class LongBench extends Benchmark
class Loogle extends Benchmark
class InfBench extends Benchmark

class BenchmarkExecutor {
  + field: type
  + evaluate(Benchmark)
  + result_storage: ResultStorage
}

class ResultStorage {
  + storage_path: String
  + store(List[String]): String
}

' === MetricLogger ==

class MicroMetricLogger <<singleton>> {
  + available_metrics: List[MicroMetric]
  + metrics_to_log: List[MicroMetric]
  + path_to_log
  + register_metric()
  + should_log_metric()
  + log(location, metric, value)
}

abstract MicroMetric {
  + name
  + compute(*args, **kwargs)
}

class TopkRecall extends MicroMetric{
  + name
  + compute(*args, **kwargs)
}

class LocalError extends MicroMetric {
  + name
  + compute(*args, **kwargs)
}

class SampleVariance extends MicroMetric {
  + name
  + compute(*args, **kwargs)
}


' === Plotting ===
class PlotGenerator {
  + storage_path: String
  + generate_plot(Granularity): String
  - generate_plot_1(Granularity): String
  - generate_plot_2(Granularity): String
}

enum Granularity {
  PER_TOKEN
  PER_HEAD
  PER_LAYER
}

' === Tester ===
class Tester {
  + execute_all_tests()
  + execute_unit_tests()
  + execute_integration_tests()
}

' ==  dependencies ==
' Core sparse attention system
SparseAttention *-- SparseAttentionMetaData
ResearchAttention o-- ResearchMasker

' New adapter system
ModelAdapterHF o-- Request
ModelAdapterHF o-- RequestResponse
ModelAdapterHF *-- SparseAttention
ModelAdapter o-- SparseAttentionConfig

' Deprecated dependencies (marked for removal)
SparseAttentionGen o-- SparseAttention
SparseAttentionServer o-- Pipeline

' Benchmarking and metrics (unchanged)
BenchmarkExecutor o-- Benchmark
BenchmarkExecutor *-- ResultStorage
MicroMetricLogger o-- MicroMetric
PlotGenerator o-- Granularity



@enduml
