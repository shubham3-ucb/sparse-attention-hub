# Add type annotations in sparse_attention
_Exported on 7/11/2025 at 14:29:03 PDT from Cursor (1.2.2)_

---

**User**

In the sparse_attention folder
Use type annotations in all the methods for all statements of the type
a = f(b)
use a: Type = f(b)

---

**Cursor**

I'll help you add type annotations to all methods in the sparse_attention folder. Let me first explore the folder structure and examine all the Python files to understand what needs to be annotated.

Now I'll read all the Python files to identify the statements that need type annotations. Let me start with the main files and then process subdirectories systematically.

```python
    batch: int
    num_key_value_heads: int
    slen: int
    head_dim: int
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape

->

    batch: int
    num_key_value_heads: int
    slen: int
    head_dim: int
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
```

```python
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(
        batch, num_key_value_heads, n_rep, slen, head_dim
    )

->

    if n_rep == 1:
        return hidden_states
    hidden_states: torch.Tensor = hidden_states[:, :, None, :, :].expand(
        batch, num_key_value_heads, n_rep, slen, head_dim
    )
```

```python
    """
    num_attention_heads: int = queries.shape[1]
    num_key_value_heads: int = kv_tensor.shape[1]

->

    """
    num_attention_heads: int = queries.shape[1]
    num_key_value_heads: int = kv_tensor.shape[1]
```

```python
    num_attention_heads: int = queries.shape[1]
    num_key_value_heads: int = kv_tensor.shape[1]
```

```python
    # Calculate num_key_value_groups from tensor shapes
    num_key_value_groups = _get_num_key_value_groups(queries, keys)

    # Apply key-value grouping if needed
    key_states = repeat_kv(keys, num_key_value_groups)

    raw_attention_weights = torch.matmul(queries, key_states.transpose(2, 3)) * scaling

->

    # Calculate num_key_value_groups from tensor shapes
    num_key_value_groups: int = _get_num_key_value_groups(queries, keys)

    # Apply key-value grouping if needed
    key_states: torch.Tensor = repeat_kv(keys, num_key_value_groups)

    raw_attention_weights: torch.Tensor = torch.matmul(queries, key_states.transpose(2, 3)) * scaling
```

```python
    if attention_mask is not None:
        raw_attention_weights = (
            raw_attention_weights + attention_mask[:, :, :, : key_states.shape[-2]]
        )

    row_wise_max = torch.max(raw_attention_weights, dim=-1, keepdim=True)[0]
    raw_attention_weights = raw_attention_weights - row_wise_max
    exp_attention_weights = torch.exp(raw_attention_weights)

->

    if attention_mask is not None:
        raw_attention_weights: torch.Tensor = (
            raw_attention_weights + attention_mask[:, :, :, : key_states.shape[-2]]
        )

    row_wise_max: torch.Tensor = torch.max(raw_attention_weights, dim=-1, keepdim=True)[0]
    raw_attention_weights: torch.Tensor = raw_attention_weights - row_wise_max
    exp_attention_weights: torch.Tensor = torch.exp(raw_attention_weights)
```

```python
    if not sparse_attention_mask.is_empty():
        exp_attention_weights = sparse_attention_mask.apply_inv_mask(
            exp_attention_weights
        )

    # Apply dropout to attention weights if specified
    if dropout > 0.0 and training:
        exp_attention_weights = torch.nn.functional.dropout(
            exp_attention_weights, p=dropout, training=training
        )

->

    if not sparse_attention_mask.is_empty():
        exp_attention_weights: torch.Tensor = sparse_attention_mask.apply_inv_mask(
            exp_attention_weights
        )

    # Apply dropout to attention weights if specified
    if dropout > 0.0 and training:
        exp_attention_weights: torch.Tensor = torch.nn.functional.dropout(
            exp_attention_weights, p=dropout, training=training
        )
```

```python
    if attention_mask is not None:
        raw_attention_weights: torch.Tensor = (
            raw_attention_weights + attention_mask[:, :, :, : key_states.shape[-2]]
        )

    row_wise_max: torch.Tensor = torch.max(raw_attention_weights, dim=-1, keepdim=True)[0]
    raw_attention_weights: torch.Tensor = raw_attention_weights - row_wise_max
    exp_attention_weights: torch.Tensor = torch.exp(raw_attention_weights)

    if not sparse_attention_mask.is_empty():
        exp_attention_weights: torch.Tensor = sparse_attention_mask.apply_inv_mask(
            exp_attention_weights
        )

    # Apply dropout to attention weights if specified
    if dropout > 0.0 and training:
        exp_attention_weights: torch.Tensor = torch.nn.functional.dropout(
            exp_attention_weights, p=dropout, training=training
        )

->

    if attention_mask is not None:
        raw_attention_weights = (
            raw_attention_weights + attention_mask[:, :, :, : key_states.shape[-2]]
        )

    row_wise_max: torch.Tensor = torch.max(raw_attention_weights, dim=-1, keepdim=True)[0]
    raw_attention_weights = raw_attention_weights - row_wise_max
    exp_attention_weights: torch.Tensor = torch.exp(raw_attention_weights)

    if not sparse_attention_mask.is_empty():
        exp_attention_weights = sparse_attention_mask.apply_inv_mask(
            exp_attention_weights
        )

    # Apply dropout to attention weights if specified
    if dropout > 0.0 and training:
        exp_attention_weights = torch.nn.functional.dropout(
            exp_attention_weights, p=dropout, training=training
        )
```

```python
    """
    training = module.training if module is not None else False
    exp_attention_weights = _compute_masked_exp_attention_weights(
        queries=queries,
        keys=keys,
        attention_mask=attention_mask,
        scaling=scaling,
        sparse_attention_mask=sparse_attention_mask,
        dropout=dropout,
        training=training,
    )

->

    """
    training: bool = module.training if module is not None else False
    exp_attention_weights: torch.Tensor = _compute_masked_exp_attention_weights(
        queries=queries,
        keys=keys,
        attention_mask=attention_mask,
        scaling=scaling,
        sparse_attention_mask=sparse_attention_mask,
        dropout=dropout,
        training=training,
    )
```

```python
    Returns:
        Denominator tensor of shape (b, h, sq, 1)
    """
    training = module.training if module is not None else False
    exp_attention_weights = _compute_masked_exp_attention_weights(
        queries=queries,
        keys=keys,
        attention_mask=attention_mask,
        scaling=scaling,
        sparse_attention_mask=sparse_attention_mask,
        dropout=dropout,
        training=training,
    )

->

    Returns:
        Denominator tensor of shape (b, h, sq, 1)
    """
    training: bool = module.training if module is not None else False
    exp_attention_weights: torch.Tensor = _compute_masked_exp_attention_weights(
        queries=queries,
        keys=keys,
        attention_mask=attention_mask,
        scaling=scaling,
        sparse_attention_mask=sparse_attention_mask,
        dropout=dropout,
        training=training,
    )
```

```python
    Returns:
        Numerator tensor of shape (b, h, sq, d)
    """
    training = module.training if module is not None else False
    exp_attention_weights = _compute_masked_exp_attention_weights(
        queries=queries,
        keys=keys,
        attention_mask=attention_mask,
        scaling=scaling,
        sparse_attention_mask=sparse_attention_mask,
        dropout=dropout,
        training=training,
    )

    # Prepare values by applying key-value grouping
    num_key_value_groups = _get_num_key_value_groups(queries, values)
    value_states = repeat_kv(values, num_key_value_groups)

->

    Returns:
        Numerator tensor of shape (b, h, sq, d)
    """
    training: bool = module.training if module is not None else False
    exp_attention_weights: torch.Tensor = _compute_masked_exp_attention_weights(
        queries=queries,
        keys=keys,
        attention_mask=attention_mask,
        scaling=scaling,
        sparse_attention_mask=sparse_attention_mask,
        dropout=dropout,
        training=training,
    )

    # Prepare values by applying key-value grouping
    num_key_value_groups: int = _get_num_key_value_groups(queries, values)
    value_states: torch.Tensor = repeat_kv(values, num_key_value_groups)
```

```python
    # Compute exponential attention weights once and reuse
    training = module.training if module is not None else False
    exp_attention_weights = _compute_masked_exp_attention_weights(
        queries=queries,
        keys=keys,
        attention_mask=attention_mask,
        scaling=scaling,
        sparse_attention_mask=sparse_attention_mask,
        dropout=dropout,
        training=training,
    )

    # Prepare values by applying key-value grouping
    num_key_value_groups = _get_num_key_value_groups(queries, values)
    value_states = repeat_kv(values, num_key_value_groups)

    # Use internal helpers with pre-computed weights
    num = _get_attention_numerator(exp_attention_weights, value_states)
    den = _get_attention_denominator(exp_attention_weights)

    # Compute final attention output
    attention_output = (num / den).transpose(1, 2).contiguous()

->

    # Compute exponential attention weights once and reuse
    training: bool = module.training if module is not None else False
    exp_attention_weights: torch.Tensor = _compute_masked_exp_attention_weights(
        queries=queries,
        keys=keys,
        attention_mask=attention_mask,
        scaling=scaling,
        sparse_attention_mask=sparse_attention_mask,
        dropout=dropout,
        training=training,
    )

    # Prepare values by applying key-value grouping
    num_key_value_groups: int = _get_num_key_value_groups(queries, values)
    value_states: torch.Tensor = repeat_kv(values, num_key_value_groups)

    # Use internal helpers with pre-computed weights
    num: torch.Tensor = _get_attention_numerator(exp_attention_weights, value_states)
    den: torch.Tensor = _get_attention_denominator(exp_attention_weights)

    # Compute final attention output
    attention_output: torch.Tensor = (num / den).transpose(1, 2).contiguous()
```

```python
    if return_attention_weights:
        # Normalize exponential weights to get attention probabilities
        attention_weights = exp_attention_weights / den
        return attention_output, attention_weights

->

    if return_attention_weights:
        # Normalize exponential weights to get attention probabilities
        attention_weights: torch.Tensor = exp_attention_weights / den
        return attention_output, attention_weights
```

```python
        # For multi-dimensional tensors, ptr should have size equal to the product of all dimensions except the last one, plus 1
        expected_ptr_size = np.prod(shape[:-1]) + 1
        if ptr.numel() != expected_ptr_size:
            raise ValueError(
                f"ptr.numel() must be {expected_ptr_size}, got {ptr.numel()}"
            )

->

        # For multi-dimensional tensors, ptr should have size equal to the product of all dimensions except the last one, plus 1
        expected_ptr_size: int = np.prod(shape[:-1]) + 1
        if ptr.numel() != expected_ptr_size:
            raise ValueError(
                f"ptr.numel() must be {expected_ptr_size}, got {ptr.numel()}"
            )
```

```python
        # Validate input shapes
        expected_row_wise_shape = shape[:-1] + (row_wise_idx.shape[-1],)
        if row_wise_idx.shape != expected_row_wise_shape:
            raise ValueError(
                f"row_wise_idx.shape must be {expected_row_wise_shape}, got {row_wise_idx.shape}"
            )

        if data.shape != row_wise_idx.shape:
            raise ValueError(
                f"data.shape must match row_wise_idx.shape {row_wise_idx.shape}, got {data.shape}"
            )

        # Validate indices are within bounds (allow -1 as padding)
        n = shape[-1]
        valid_indices_mask = row_wise_idx >= 0
        if torch.any((row_wise_idx >= n) & valid_indices_mask):
            raise ValueError(
                f"All valid indices in row_wise_idx must be in range [0, {n-1}]"
            )

->

        # Validate input shapes
        expected_row_wise_shape: Tuple[int, ...] = shape[:-1] + (row_wise_idx.shape[-1],)
        if row_wise_idx.shape != expected_row_wise_shape:
            raise ValueError(
                f"row_wise_idx.shape must be {expected_row_wise_shape}, got {row_wise_idx.shape}"
            )

        if data.shape != row_wise_idx.shape:
            raise ValueError(
                f"data.shape must match row_wise_idx.shape {row_wise_idx.shape}, got {data.shape}"
            )

        # Validate indices are within bounds (allow -1 as padding)
        n: int = shape[-1]
        valid_indices_mask: torch.Tensor = row_wise_idx >= 0
        if torch.any((row_wise_idx >= n) & valid_indices_mask):
            raise ValueError(
                f"All valid indices in row_wise_idx must be in range [0, {n-1}]"
            )
```

```python
        # Always compute sparse representation first
        batch_dims = shape[:-1]
        batch_size = int(np.prod(batch_dims))
        k = row_wise_idx.shape[-1]

        flat_row_wise_idx = row_wise_idx.reshape(batch_size, k)
        flat_data = data.reshape(batch_size, k)

        row_offsets = (
            torch.arange(batch_size, device=flat_row_wise_idx.device).unsqueeze(1) * n
        )
        flat_indices_with_offset = flat_row_wise_idx + row_offsets

        # Filter out invalid indices (e.g., -1 padding)
        valid_mask = flat_row_wise_idx >= 0
        valid_flat_indices = flat_indices_with_offset[valid_mask]
        valid_values = flat_data[valid_mask]

        valid_counts_per_row = valid_mask.sum(dim=1)
        ptr = torch.cat(
            [
                torch.zeros(1, dtype=torch.long, device=flat_row_wise_idx.device),
                torch.cumsum(valid_counts_per_row, dim=0),
            ]
        )

->

        # Always compute sparse representation first
        batch_dims: Tuple[int, ...] = shape[:-1]
        batch_size: int = int(np.prod(batch_dims))
        k: int = row_wise_idx.shape[-1]

        flat_row_wise_idx: torch.Tensor = row_wise_idx.reshape(batch_size, k)
        flat_data: torch.Tensor = data.reshape(batch_size, k)

        row_offsets: torch.Tensor = (
            torch.arange(batch_size, device=flat_row_wise_idx.device).unsqueeze(1) * n
        )
        flat_indices_with_offset: torch.Tensor = flat_row_wise_idx + row_offsets

        # Filter out invalid indices (e.g., -1 padding)
        valid_mask: torch.Tensor = flat_row_wise_idx >= 0
        valid_flat_indices: torch.Tensor = flat_indices_with_offset[valid_mask]
        valid_values: torch.Tensor = flat_data[valid_mask]

        valid_counts_per_row: torch.Tensor = valid_mask.sum(dim=1)
        ptr: torch.Tensor = torch.cat(
            [
                torch.zeros(1, dtype=torch.long, device=flat_row_wise_idx.device),
                torch.cumsum(valid_counts_per_row, dim=0),
            ]
        )
```

```python
        # Create sparse mask
        sparse_mask = cls.create_mask_from_indices(
            shape, valid_flat_indices, ptr, valid_values, dtype
        )

        if type == "dense":
            # Convert to dense representation
            dense_mask = sparse_mask.get_dense_mask()
            return cls.create_mask_from_dense_mask(shape, dense_mask, dtype)
        elif type == "index":
            return sparse_mask
        else:
            raise ValueError(f"type must be 'index' or 'dense', got {type}")

->

        # Create sparse mask
        sparse_mask: Mask = cls.create_mask_from_indices(
            shape, valid_flat_indices, ptr, valid_values, dtype
        )

        if type == "dense":
            # Convert to dense representation
            dense_mask: torch.Tensor = sparse_mask.get_dense_mask()
            return cls.create_mask_from_dense_mask(shape, dense_mask, dtype)
        elif type == "index":
            return sparse_mask
        else:
            raise ValueError(f"type must be 'index' or 'dense', got {type}")
```

```python
        if self.is_full:
            # For full masks, generate all indices with value 1.0
            total_size = int(np.prod(self.shape))
            indices = torch.arange(total_size, dtype=torch.long)
            data = torch.ones(total_size, dtype=self.dtype)

            # Create ptr array
            n_cols = self.shape[-1]
            ptr = torch.arange(0, total_size + 1, n_cols, dtype=torch.long)

            return indices, ptr, data

->

        if self.is_full:
            # For full masks, generate all indices with value 1.0
            total_size: int = int(np.prod(self.shape))
            indices: torch.Tensor = torch.arange(total_size, dtype=torch.long)
            data: torch.Tensor = torch.ones(total_size, dtype=self.dtype)

            # Create ptr array
            n_cols: int = self.shape[-1]
            ptr: torch.Tensor = torch.arange(0, total_size + 1, n_cols, dtype=torch.long)

            return indices, ptr, data
```

```python
            if self.data is None:
                # If no data is provided, assume 1.0 values
                data = torch.ones(
                    self.indices.numel(), dtype=self.dtype, device=self.indices.device
                )
            else:
                data = self.data
            return self.indices, self.ptr, data

->

            if self.data is None:
                # If no data is provided, assume 1.0 values
                data: torch.Tensor = torch.ones(
                    self.indices.numel(), dtype=self.dtype, device=self.indices.device
                )
            else:
                data: torch.Tensor = self.data
            return self.indices, self.ptr, data
```

```python
            # Convert dense representation to sparse representation
            # Find non-zero indices
            non_zero_indices = torch.nonzero(
                self.mask.view(-1), as_tuple=False
            ).squeeze(-1)
            data = self.mask.view(-1)[non_zero_indices]

            # Create ptr array with size equal to product of all dimensions except the last one, plus 1
            # Vectorized version: count nonzero per row and cumsum
            counts = torch.count_nonzero(self.mask.view(-1, self.shape[-1]), dim=1)
            ptr = torch.cat(
                [
                    torch.zeros(1, dtype=torch.long, device=self.mask.device),
                    torch.cumsum(counts, dim=0),
                ]
            )

->

            # Convert dense representation to sparse representation
            # Find non-zero indices
            non_zero_indices: torch.Tensor = torch.nonzero(
                self.mask.view(-1), as_tuple=False
            ).squeeze(-1)
            data: torch.Tensor = self.mask.view(-1)[non_zero_indices]

            # Create ptr array with size equal to product of all dimensions except the last one, plus 1
            # Vectorized version: count nonzero per row and cumsum
            counts: torch.Tensor = torch.count_nonzero(self.mask.view(-1, self.shape[-1]), dim=1)
            ptr: torch.Tensor = torch.cat(
                [
                    torch.zeros(1, dtype=torch.long, device=self.mask.device),
                    torch.cumsum(counts, dim=0),
                ]
            )
```

```python
            # Convert sparse representation to dense mask
            # Ensure dense mask is on the same device as indices/data
            device = self.indices.device
            dense_mask = torch.zeros(self.shape, dtype=self.dtype, device=device)
            if len(self.indices) > 0:
                if self.data is not None:
                    dense_mask.view(-1)[self.indices] = self.data
                else:
                    dense_mask.view(-1)[self.indices] = 1.0
            return dense_mask

->

            # Convert sparse representation to dense mask
            # Ensure dense mask is on the same device as indices/data
            device: torch.device = self.indices.device
            dense_mask: torch.Tensor = torch.zeros(self.shape, dtype=self.dtype, device=device)
            if len(self.indices) > 0:
                if self.data is not None:
                    dense_mask.view(-1)[self.indices] = self.data
                else:
                    dense_mask.view(-1)[self.indices] = 1.0
            return dense_mask
```

```python
        if self.from_dense_mask:
            if self.mask is None:
                raise RuntimeError("Dense mask is None")
            mask = self.mask
        elif self.from_index:
            mask = self.get_dense_mask()
        else:
            raise RuntimeError("Mask object is in an invalid state")

        # Create output tensor
        output = torch.zeros_like(input_tensor)

        # Apply inverse mask logic
        non_zero_mask = mask != 0
        output[non_zero_mask] = input_tensor[non_zero_mask] * (
            1.0 / mask[non_zero_mask]
        )

->

        if self.from_dense_mask:
            if self.mask is None:
                raise RuntimeError("Dense mask is None")
            mask: torch.Tensor = self.mask
        elif self.from_index:
            mask: torch.Tensor = self.get_dense_mask()
        else:
            raise RuntimeError("Mask object is in an invalid state")

        # Create output tensor
        output: torch.Tensor = torch.zeros_like(input_tensor)

        # Apply inverse mask logic
        non_zero_mask: torch.Tensor = mask != 0
        output[non_zero_mask] = input_tensor[non_zero_mask] * (
            1.0 / mask[non_zero_mask]
        )
```

```python
        if mask_type == "dense":
            empty_mask = torch.zeros(shape, dtype=dtype)
            return cls.create_mask_from_dense_mask(shape, empty_mask, dtype)
        elif mask_type == "index":
            # For an empty mask, we have no non-zero indices
            empty_indices = torch.empty(0, dtype=torch.long)

            # Create ptr array with size equal to product of all dimensions except the last one, plus 1
            # All rows have 0 non-zero elements, so ptr is just zeros
            ptr_size = int(np.prod(shape[:-1]) + 1)
            ptr = torch.zeros(ptr_size, dtype=torch.long)

            # No data since all values are zero
            data = torch.empty(0, dtype=dtype)

            return cls.create_mask_from_indices(shape, empty_indices, ptr, data, dtype)
        else:
            raise ValueError(f"mask_type must be 'dense' or 'index', got {mask_type}")

->

        if mask_type == "dense":
            empty_mask: torch.Tensor = torch.zeros(shape, dtype=dtype)
            return cls.create_mask_from_dense_mask(shape, empty_mask, dtype)
        elif mask_type == "index":
            # For an empty mask, we have no non-zero indices
            empty_indices: torch.Tensor = torch.empty(0, dtype=torch.long)

            # Create ptr array with size equal to product of all dimensions except the last one, plus 1
            # All rows have 0 non-zero elements, so ptr is just zeros
            ptr_size: int = int(np.prod(shape[:-1]) + 1)
            ptr: torch.Tensor = torch.zeros(ptr_size, dtype=torch.long)

            # No data since all values are zero
            data: torch.Tensor = torch.empty(0, dtype=dtype)

            return cls.create_mask_from_indices(shape, empty_indices, ptr, data, dtype)
        else:
            raise ValueError(f"mask_type must be 'dense' or 'index', got {mask_type}")
```

```python
        self_indices, self_ptr, self_data = self.get_index_mask()
        other_indices, other_ptr, other_data = other_mask.get_index_mask()

        device = (
            self_indices.device
            if self_indices.numel() > 0
            else (
                other_indices.device
                if other_indices.numel() > 0
                else torch.device("cpu")
            )
        )

->

        self_indices: torch.Tensor
        self_ptr: torch.Tensor
        self_data: torch.Tensor
        self_indices, self_ptr, self_data = self.get_index_mask()
        other_indices: torch.Tensor
        other_ptr: torch.Tensor
        other_data: torch.Tensor
        other_indices, other_ptr, other_data = other_mask.get_index_mask()

        device: torch.device = (
            self_indices.device
            if self_indices.numel() > 0
            else (
                other_indices.device
                if other_indices.numel() > 0
                else torch.device("cpu")
            )
        )
```

```python
        if self_indices.numel() == 0 and other_indices.numel() == 0:
            final_indices = torch.empty(0, dtype=torch.long, device=device)
            final_data = torch.empty(0, dtype=self.dtype, device=device)
            num_rows = int(np.prod(self.shape[:-1]))
            final_ptr = torch.zeros(num_rows + 1, dtype=torch.long, device=device)
        elif self_indices.numel() == 0:
            final_indices = other_indices
            final_data = torch.clamp(other_data, 0.0, 1.0)
            final_ptr = other_ptr
        elif other_indices.numel() == 0:
            final_indices = self_indices
            final_data = torch.clamp(self_data, 0.0, 1.0)
            final_ptr = self_ptr
        else:
            all_indices = torch.cat([self_indices, other_indices])
            all_data = torch.cat([self_data, other_data])

            sorted_indices, sort_order = torch.sort(all_indices)
            sorted_data = all_data[sort_order]

            unique_indices, inverse_indices = torch.unique_consecutive(
                sorted_indices, return_inverse=True
            )

            summed_data = torch.zeros(
                unique_indices.numel(), dtype=self.dtype, device=device
            )
            summed_data.scatter_add_(0, inverse_indices, sorted_data)

            final_data = torch.clamp(summed_data, 0.0, 1.0)
            final_indices = unique_indices

            # Reconstruct ptr array by counting elements per row
            num_rows = int(np.prod(self.shape[:-1]))
            n_cols = self.shape[-1]
            row_indices = final_indices // n_cols
            row_counts = torch.bincount(row_indices, minlength=num_rows)
            final_ptr = torch.cat(
                [
                    torch.zeros(1, dtype=torch.long, device=device),
                    torch.cumsum(row_counts, dim=0),
                ]
            )

->

        if self_indices.numel() == 0 and other_indices.numel() == 0:
            final_indices: torch.Tensor = torch.empty(0, dtype=torch.long, device=device)
            final_data: torch.Tensor = torch.empty(0, dtype=self.dtype, device=device)
            num_rows: int = int(np.prod(self.shape[:-1]))
            final_ptr: torch.Tensor = torch.zeros(num_rows + 1, dtype=torch.long, device=device)
        elif self_indices.numel() == 0:
            final_indices: torch.Tensor = other_indices
            final_data: torch.Tensor = torch.clamp(other_data, 0.0, 1.0)
            final_ptr: torch.Tensor = other_ptr
        elif other_indices.numel() == 0:
            final_indices: torch.Tensor = self_indices
            final_data: torch.Tensor = torch.clamp(self_data, 0.0, 1.0)
            final_ptr: torch.Tensor = self_ptr
        else:
            all_indices: torch.Tensor = torch.cat([self_indices, other_indices])
            all_data: torch.Tensor = torch.cat([self_data, other_data])

            sorted_indices: torch.Tensor
            sort_order: torch.Tensor
            sorted_indices, sort_order = torch.sort(all_indices)
            sorted_data: torch.Tensor = all_data[sort_order]

            unique_indices: torch.Tensor
            inverse_indices: torch.Tensor
            unique_indices, inverse_indices = torch.unique_consecutive(
                sorted_indices, return_inverse=True
            )

            summed_data: torch.Tensor = torch.zeros(
                unique_indices.numel(), dtype=self.dtype, device=device
            )
            summed_data.scatter_add_(0, inverse_indices, sorted_data)

            final_data: torch.Tensor = torch.clamp(summed_data, 0.0, 1.0)
            final_indices: torch.Tensor = unique_indices

            # Reconstruct ptr array by counting elements per row
            num_rows: int = int(np.prod(self.shape[:-1]))
            n_cols: int = self.shape[-1]
            row_indices: torch.Tensor = final_indices // n_cols
            row_counts: torch.Tensor = torch.bincount(row_indices, minlength=num_rows)
            final_ptr: torch.Tensor = torch.cat(
                [
                    torch.zeros(1, dtype=torch.long, device=device),
                    torch.cumsum(row_counts, dim=0),
                ]
            )
```

```python
        # Check if the result is a full mask
        expected_size = int(np.prod(self.shape))
        if final_indices.numel() == expected_size and torch.all(final_data == 1.0):
            # Result is a full mask, use optimization
            if inplace:
                self.is_full = True
                self.from_dense_mask = False
                self.from_index = False
                self.mask = None
                self.indices = None
                self.ptr = None
                self.data = None
                return self
            else:
                return Mask.create_full_mask(self.shape, self.dtype)

->

        # Check if the result is a full mask
        expected_size: int = int(np.prod(self.shape))
        if final_indices.numel() == expected_size and torch.all(final_data == 1.0):
            # Result is a full mask, use optimization
            if inplace:
                self.is_full = True
                self.from_dense_mask = False
                self.from_index = False
                self.mask = None
                self.indices = None
                self.ptr = None
                self.data = None
                return self
            else:
                return Mask.create_full_mask(self.shape, self.dtype)
```

```python
        # For multi-dimensional tensors, ptr should have size equal to the product of all dimensions except the last one, plus 1
        expected_ptr_size: int = np.prod(shape[:-1]) + 1

->

        # For multi-dimensional tensors, ptr should have size equal to the product of all dimensions except the last one, plus 1
        expected_ptr_size: int = int(np.prod(shape[:-1]) + 1)
```

```python
        elif self_indices.numel() == 0:
            final_indices: torch.Tensor = other_indices
            final_data: torch.Tensor = torch.clamp(other_data, 0.0, 1.0)
            final_ptr: torch.Tensor = other_ptr
        elif other_indices.numel() == 0:
            final_indices: torch.Tensor = self_indices
            final_data: torch.Tensor = torch.clamp(self_data, 0.0, 1.0)
            final_ptr: torch.Tensor = self_ptr

->

        elif self_indices.numel() == 0:
            final_indices = other_indices
            final_data = torch.clamp(other_data, 0.0, 1.0)
            final_ptr = other_ptr
        elif other_indices.numel() == 0:
            final_indices = self_indices
            final_data = torch.clamp(self_data, 0.0, 1.0)
            final_ptr = self_ptr
```

```python
            final_data: torch.Tensor = torch.clamp(summed_data, 0.0, 1.0)
            final_indices: torch.Tensor = unique_indices

->

            final_data = torch.clamp(summed_data, 0.0, 1.0)
            final_indices = unique_indices
```

```python
        """
        sparse_attention = SparseAttention.create_from_config(config)
        return cls(sparse_attention)

->

        """
        sparse_attention: SparseAttention = SparseAttention.create_from_config(config)
        return cls(sparse_attention)
```

```python
            if "sparse_meta_data" in kwargs:
                sparse_meta_data = kwargs["sparse_meta_data"]
                kwargs.pop("sparse_meta_data", None)

            else:
                raise ValueError(
                    "sparse_meta_data must be provided while calling model.forward()"
                )

->

            if "sparse_meta_data" in kwargs:
                sparse_meta_data: Dict[Any, Any] = kwargs["sparse_meta_data"]
                kwargs.pop("sparse_meta_data", None)

            else:
                raise ValueError(
                    "sparse_meta_data must be provided while calling model.forward()"
                )
```

```python
        """Generate a unique name not present in ALL_ATTENTION_FUNCTIONS."""
        base_name: str = "sparse_attention"
        existing_keys: List[str] = (
            ALL_ATTENTION_FUNCTIONS.valid_keys()
            + ALL_MASK_ATTENTION_FUNCTIONS.valid_keys()
        )

        while True:
            suffix: str = "".join(
                random.choices(string.ascii_lowercase + string.digits, k=8)
            )
            name: str = f"{base_name}_{suffix}"

            if name not in existing_keys:
                return name

->

        """Generate a unique name not present in ALL_ATTENTION_FUNCTIONS."""
        base_name: str = "sparse_attention"
        existing_keys: List[str] = (
            ALL_ATTENTION_FUNCTIONS.valid_keys()
            + ALL_MASK_ATTENTION_FUNCTIONS.valid_keys()
        )

        while True:
            suffix: str = "".join(
                random.choices(string.ascii_lowercase + string.digits, k=8)
            )
            name: str = f"{base_name}_{suffix}"

            if name not in existing_keys:
                return name
```

```python
        try:
            custom_attention_fn: Callable = self.get_custom_attention_function()
            custom_attention_name = self._generate_unique_attention_name()
            from transformers.masking_utils import eager_mask

->

        try:
            custom_attention_fn: Callable = self.get_custom_attention_function()
            custom_attention_name: str = self._generate_unique_attention_name()
            from transformers.masking_utils import eager_mask
```

```python
        # Validate that there's at most one sampling masker
        sampling_masker_count = sum(
            1 for masker in maskers if isinstance(masker, SamplingMasker)
        )
        if sampling_masker_count > 1:
            raise ValueError(
                "Only one sampling masker supported for efficiency; consider implementing all sampling logic in one masker"
            )

->

        # Validate that there's at most one sampling masker
        sampling_masker_count: int = sum(
            1 for masker in maskers if isinstance(masker, SamplingMasker)
        )
        if sampling_masker_count > 1:
            raise ValueError(
                "Only one sampling masker supported for efficiency; consider implementing all sampling logic in one masker"
            )
```

```python
        # Create an empty Mask object
        mask_shape = (
            queries.shape[0],
            queries.shape[1],
            queries.shape[2],
            keys.shape[2],
        )
        sparse_attention_mask = Mask.create_empty_mask(mask_shape, dtype=queries.dtype)

        # Apply all maskers sequentially, each one on the output of the previous one
        for masker in self.maskers:
            sparse_attention_mask = masker.add_mask(
                keys=keys,
                queries=queries,
                values=values,
                attention_mask=attention_mask,
                sparse_meta_data=sparse_meta_data,
                previous_mask=sparse_attention_mask,
                **kwargs,
            )

->

        # Create an empty Mask object
        mask_shape: Tuple[int, int, int, int] = (
            queries.shape[0],
            queries.shape[1],
            queries.shape[2],
            keys.shape[2],
        )
        sparse_attention_mask: Mask = Mask.create_empty_mask(mask_shape, dtype=queries.dtype)

        # Apply all maskers sequentially, each one on the output of the previous one
        for masker in self.maskers:
            sparse_attention_mask = masker.add_mask(
                keys=keys,
                queries=queries,
                values=values,
                attention_mask=attention_mask,
                sparse_meta_data=sparse_meta_data,
                previous_mask=sparse_attention_mask,
                **kwargs,
            )
```

```python
        # Call compute_masked_attention_output on the result of the last mask
        # Always request attention weights to match the expected return signature
        attention_output, attention_weights = get_masked_attention_output(
            module=module,
            queries=queries,
            keys=keys,
            values=values,
            attention_mask=attention_mask,
            scaling=scaling,
            dropout=dropout,
            sparse_attention_mask=sparse_attention_mask,
            return_attention_weights=True,
            **kwargs,
        )

->

        # Call compute_masked_attention_output on the result of the last mask
        # Always request attention weights to match the expected return signature
        attention_output: torch.Tensor
        attention_weights: torch.Tensor
        attention_output, attention_weights = get_masked_attention_output(
            module=module,
            queries=queries,
            keys=keys,
            values=values,
            attention_mask=attention_mask,
            scaling=scaling,
            dropout=dropout,
            sparse_attention_mask=sparse_attention_mask,
            return_attention_weights=True,
            **kwargs,
        )
```

```python
        # Create ResearchMasker objects from the configs using the factory method
        maskers = []
        for masker_config in config.masker_configs:
            masker = ResearchMasker.create_masker_from_config(masker_config)
            maskers.append(masker)

->

        # Create ResearchMasker objects from the configs using the factory method
        maskers: List[ResearchMasker] = []
        for masker_config in config.masker_configs:
            masker: ResearchMasker = ResearchMasker.create_masker_from_config(masker_config)
            maskers.append(masker)
```

```python
        # Look up the concrete class based on the config type
        concrete_class: Optional[Type[EfficientAttention]] = _EFFICIENT_ATTENTION_REGISTRY.get(type(config))
        if concrete_class is None:
            raise ValueError(
                f"No efficient attention class found for config type: {type(config)}"
            )

        # Cast to help mypy understand the type
        concrete_class = cast(Type[EfficientAttention], concrete_class)
        # Call the concrete class's create_from_config method
        return concrete_class.create_from_config(config)

->

        # Look up the concrete class based on the config type
        concrete_class: Optional[Type[EfficientAttention]] = _EFFICIENT_ATTENTION_REGISTRY.get(type(config))
        if concrete_class is None:
            raise ValueError(
                f"No efficient attention class found for config type: {type(config)}"
            )

        # Cast to help mypy understand the type
        concrete_class: Type[EfficientAttention] = cast(Type[EfficientAttention], concrete_class)
        # Call the concrete class's create_from_config method
        return concrete_class.create_from_config(config)
```

```python
        """
        masker_class = cls._registry.get(config_type)
        if masker_class is None:
            raise ValueError(
                f"No masker class registered for config type: {config_type}"
            )
        return masker_class

->

        """
        masker_class: Optional[Type["ResearchMasker"]] = cls._registry.get(config_type)
        if masker_class is None:
            raise ValueError(
                f"No masker class registered for config type: {config_type}"
            )
        return masker_class
```

```python
from typing import Any, Callable, Dict, Tuple, Type, TypeVar, Union, cast

->

from typing import Any, Callable, Dict, Optional, Tuple, Type, TypeVar, Union, cast
```

```python
        """
        return AttentionTensorDimensions(
            batch_size=queries.shape[0],
            num_heads=queries.shape[1],
            seq_len_queries=queries.shape[2],
            seq_len_keys=keys.shape[2],
        )

->

        """
        return AttentionTensorDimensions(
            batch_size=queries.shape[0],
            num_heads=queries.shape[1],
            seq_len_queries=queries.shape[2],
            seq_len_keys=keys.shape[2],
        )
```

```python
        """Create a full attention mask."""
        mask_shape = (
            dims.batch_size,
            dims.num_heads,
            dims.seq_len_queries,
            dims.seq_len_keys,
        )
        return Mask.create_full_mask(mask_shape, dtype=dtype)

->

        """Create a full attention mask."""
        mask_shape: Tuple[int, int, int, int] = (
            dims.batch_size,
            dims.num_heads,
            dims.seq_len_queries,
            dims.seq_len_keys,
        )
        return Mask.create_full_mask(mask_shape, dtype=dtype)
```

```python
        """Create mask from row-wise indices."""
        mask_shape: Tuple[int, int, int, int] = (
            dims.batch_size,
            dims.num_heads,
            dims.seq_len_queries,
            dims.seq_len_keys,
        )
        data: torch.Tensor = torch.ones_like(indices, dtype=dtype, device=device)

        return Mask.create_from_row_wise_idx(
            shape=mask_shape, row_wise_idx=indices, data=data, type="index", dtype=dtype
        )

->

        """Create mask from row-wise indices."""
        mask_shape: Tuple[int, int, int, int] = (
            dims.batch_size,
            dims.num_heads,
            dims.seq_len_queries,
            dims.seq_len_keys,
        )
        data: torch.Tensor = torch.ones_like(indices, dtype=dtype, device=device)

        return Mask.create_from_row_wise_idx(
            shape=mask_shape, row_wise_idx=indices, data=data, type="index", dtype=dtype
        )
```

```python
        """Calculate effective size based on configuration (supports both int and float)."""
        if isinstance(size_param, float):
            return int(size_param * seq_len_keys)
        return int(size_param)

->

        """Calculate effective size based on configuration (supports both int and float)."""
        if isinstance(size_param, float):
            return int(size_param * seq_len_keys)
        return int(size_param)
```

```python
        # Get the masker class from the registry
        masker_class: Type["ResearchMasker"] = MaskerRegistry.get_masker_class(type(config))

        # Cast to help mypy understand the type
        masker_class = cast(Type[ResearchMasker], masker_class)
        return masker_class.create_from_config(config)

->

        # Get the masker class from the registry
        masker_class: Type["ResearchMasker"] = MaskerRegistry.get_masker_class(type(config))

        # Cast to help mypy understand the type
        masker_class: Type[ResearchMasker] = cast(Type[ResearchMasker], masker_class)
        return masker_class.create_from_config(config)
```

```python
        """Get top-K indices from positions not already active in previous mask."""
        previous_dense_mask: torch.Tensor = previous_mask.get_dense_mask()
        masked_scores: torch.Tensor = scores.clone()
        masked_scores[previous_dense_mask != 0] = float("-inf")

        _: torch.Tensor
        top_k_indices: torch.Tensor
        _, top_k_indices = torch.topk(masked_scores, k=k, dim=-1, largest=True)
        return top_k_indices

->

        """Get top-K indices from positions not already active in previous mask."""
        previous_dense_mask: torch.Tensor = previous_mask.get_dense_mask()
        masked_scores: torch.Tensor = scores.clone()
        masked_scores[previous_dense_mask != 0] = float("-inf")

        _: torch.Tensor
        top_k_indices: torch.Tensor
        _, top_k_indices = torch.topk(masked_scores, k=k, dim=-1, largest=True)
        return top_k_indices
```

```python
        """Add local mask to enable local attention pattern."""
        if previous_mask.is_full_mask():
            return previous_mask

        tensor_dims = self._extract_tensor_dimensions(keys, queries)
        effective_window_size = self._calculate_effective_window_size(
            tensor_dims.seq_len_keys
        )

        # If sequence is small enough, use full attention
        if self._should_use_full_attention(tensor_dims, effective_window_size):
            return self._create_full_mask(tensor_dims, previous_mask.dtype)

        # Create local attention mask
        local_mask = self._create_local_mask(
            tensor_dims, effective_window_size, keys.device, previous_mask.dtype
        )
        return previous_mask.merge_mask(local_mask, inplace=False)

->

        """Add local mask to enable local attention pattern."""
        if previous_mask.is_full_mask():
            return previous_mask

        tensor_dims: AttentionTensorDimensions = self._extract_tensor_dimensions(keys, queries)
        effective_window_size: int = self._calculate_effective_window_size(
            tensor_dims.seq_len_keys
        )

        # If sequence is small enough, use full attention
        if self._should_use_full_attention(tensor_dims, effective_window_size):
            return self._create_full_mask(tensor_dims, previous_mask.dtype)

        # Create local attention mask
        local_mask: Mask = self._create_local_mask(
            tensor_dims, effective_window_size, keys.device, previous_mask.dtype
        )
        return previous_mask.merge_mask(local_mask, inplace=False)
```

```python
        """Create local attention mask using vectorized computation."""
        window_indices = self._compute_window_indices(dims, window_size, device)
        return self._create_mask_from_rowise_indices(
            dims, window_indices, device, dtype
        )

->

        """Create local attention mask using vectorized computation."""
        window_indices: torch.Tensor = self._compute_window_indices(dims, window_size, device)
        return self._create_mask_from_rowise_indices(
            dims, window_indices, device, dtype
        )
```

```python
        """Compute window indices for local attention using vectorized operations."""
        # For each query position, compute the start of its local window
        # Formula: window_start = seq_len_keys - (seq_len_queries - query_pos) - window_size + 1
        query_positions: torch.Tensor = torch.arange(
            dims.seq_len_queries, device=device, dtype=torch.long
        )
        window_starts: torch.Tensor = (
            dims.seq_len_keys - dims.seq_len_queries - window_size + query_positions + 1
        )

        # Create offset indices for the window
        window_offsets: torch.Tensor = torch.arange(window_size, device=device, dtype=torch.long)

        # Compute all window indices: window_start + offset for each query
        window_indices: torch.Tensor = window_starts.unsqueeze(1) + window_offsets.unsqueeze(0)
        window_indices = torch.clamp(window_indices, 0, dims.seq_len_keys - 1)

->

        """Compute window indices for local attention using vectorized operations."""
        # For each query position, compute the start of its local window
        # Formula: window_start = seq_len_keys - (seq_len_queries - query_pos) - window_size + 1
        query_positions: torch.Tensor = torch.arange(
            dims.seq_len_queries, device=device, dtype=torch.long
        )
        window_starts: torch.Tensor = (
            dims.seq_len_keys - dims.seq_len_queries - window_size + query_positions + 1
        )

        # Create offset indices for the window
        window_offsets: torch.Tensor = torch.arange(window_size, device=device, dtype=torch.long)

        # Compute all window indices: window_start + offset for each query
        window_indices: torch.Tensor = window_starts.unsqueeze(1) + window_offsets.unsqueeze(0)
        window_indices = torch.clamp(window_indices, 0, dims.seq_len_keys - 1)
```

```python
        """Add sink mask to enable sink attention pattern."""
        if previous_mask.is_full_mask():
            return previous_mask

        tensor_dims = self._extract_tensor_dimensions(keys, queries)
        effective_sink_size = self._calculate_effective_sink_size(
            tensor_dims.seq_len_keys
        )

        # If sequence is small enough, use full attention
        if self._should_use_full_attention(tensor_dims, effective_sink_size):
            return self._create_full_mask(tensor_dims, previous_mask.dtype)

        # Create sink attention mask
        sink_mask = self._create_sink_mask(
            tensor_dims, effective_sink_size, keys.device, previous_mask.dtype
        )
        return previous_mask.merge_mask(sink_mask, inplace=False)

->

        """Add sink mask to enable sink attention pattern."""
        if previous_mask.is_full_mask():
            return previous_mask

        tensor_dims: AttentionTensorDimensions = self._extract_tensor_dimensions(keys, queries)
        effective_sink_size: int = self._calculate_effective_sink_size(
            tensor_dims.seq_len_keys
        )

        # If sequence is small enough, use full attention
        if self._should_use_full_attention(tensor_dims, effective_sink_size):
            return self._create_full_mask(tensor_dims, previous_mask.dtype)

        # Create sink attention mask
        sink_mask: Mask = self._create_sink_mask(
            tensor_dims, effective_sink_size, keys.device, previous_mask.dtype
        )
        return previous_mask.merge_mask(sink_mask, inplace=False)
```

```python
        """Create sink attention mask using vectorized computation."""
        sink_indices = self._compute_sink_indices(dims, sink_size, device)
        return self._create_mask_from_rowise_indices(dims, sink_indices, device, dtype)

->

        """Create sink attention mask using vectorized computation."""
        sink_indices: torch.Tensor = self._compute_sink_indices(dims, sink_size, device)
        return self._create_mask_from_rowise_indices(dims, sink_indices, device, dtype)
```

```python
        """Compute sink indices for sink attention pattern."""
        # Create row_wise_idx with shape (b, h, sq, sink_size)
        # Each row contains indices [0, 1, ..., sink_size-1] (first sink_size positions)
        sink_indices: torch.Tensor = (
            torch.arange(sink_size, device=device, dtype=torch.long)
            .unsqueeze(0)
            .unsqueeze(0)
            .unsqueeze(0)
            .expand(dims.batch_size, dims.num_heads, dims.seq_len_queries, sink_size)
        )

        return sink_indices

->

        """Compute sink indices for sink attention pattern."""
        # Create row_wise_idx with shape (b, h, sq, sink_size)
        # Each row contains indices [0, 1, ..., sink_size-1] (first sink_size positions)
        sink_indices: torch.Tensor = (
            torch.arange(sink_size, device=device, dtype=torch.long)
            .unsqueeze(0)
            .unsqueeze(0)
            .unsqueeze(0)
            .expand(dims.batch_size, dims.num_heads, dims.seq_len_queries, sink_size)
        )

        return sink_indices
```

```python
        """Add oracle top-K mask to enable oracle-based attention selection."""
        if previous_mask.is_full_mask():
            return previous_mask

        tensor_dims = self._extract_tensor_dimensions(keys, queries)
        effective_heavy_size = self._calculate_effective_heavy_size(
            tensor_dims.seq_len_keys
        )

        # If sequence is small enough, use full attention
        if self._should_use_full_attention(tensor_dims, effective_heavy_size):
            return self._create_full_mask(tensor_dims, previous_mask.dtype)

        # Create oracle top-K mask
        oracle_mask = self._create_oracle_topk_mask(
            tensor_dims, effective_heavy_size, keys, queries, previous_mask
        )
        return previous_mask.merge_mask(oracle_mask, inplace=False)

->

        """Add oracle top-K mask to enable oracle-based attention selection."""
        if previous_mask.is_full_mask():
            return previous_mask

        tensor_dims: AttentionTensorDimensions = self._extract_tensor_dimensions(keys, queries)
        effective_heavy_size: int = self._calculate_effective_heavy_size(
            tensor_dims.seq_len_keys
        )

        # If sequence is small enough, use full attention
        if self._should_use_full_attention(tensor_dims, effective_heavy_size):
            return self._create_full_mask(tensor_dims, previous_mask.dtype)

        # Create oracle top-K mask
        oracle_mask: Mask = self._create_oracle_topk_mask(
            tensor_dims, effective_heavy_size, keys, queries, previous_mask
        )
        return previous_mask.merge_mask(oracle_mask, inplace=False)
```

```python
        """Create oracle top-K mask using raw attention scores."""
        raw_attention_scores = self._compute_raw_attention_scores(keys, queries)
        top_k_indices = self._get_topk_indices_from_inactive_positions(
            raw_attention_scores, previous_mask, heavy_size
        )
        return self._create_mask_from_rowise_indices(
            dims, top_k_indices, keys.device, previous_mask.dtype
        )

->

        """Create oracle top-K mask using raw attention scores."""
        raw_attention_scores: torch.Tensor = self._compute_raw_attention_scores(keys, queries)
        top_k_indices: torch.Tensor = self._get_topk_indices_from_inactive_positions(
            raw_attention_scores, previous_mask, heavy_size
        )
        return self._create_mask_from_rowise_indices(
            dims, top_k_indices, keys.device, previous_mask.dtype
        )
```

```python
        """Add hash attention top-K mask to enable hash-based attention selection."""
        layer_idx = self._validate_inputs(sparse_meta_data, kwargs)

        if previous_mask.is_full_mask():
            return previous_mask

        tensor_dims = self._extract_tensor_dimensions(keys, queries)
        effective_heavy_size = self._calculate_effective_heavy_size(
            tensor_dims.seq_len_keys
        )

        if self._should_use_full_attention(tensor_dims, effective_heavy_size):
            return self._create_full_mask(tensor_dims, previous_mask.dtype)

        remaining_kwargs = {k: v for k, v in kwargs.items() if k != "layer_idx"}
        hash_mask = self._create_hash_topk_mask(
            tensor_dims,
            effective_heavy_size,
            keys,
            queries,
            sparse_meta_data,
            previous_mask,
            layer_idx,
            **remaining_kwargs,
        )
        return previous_mask.merge_mask(hash_mask, inplace=False)

->

        """Add hash attention top-K mask to enable hash-based attention selection."""
        layer_idx: int = self._validate_inputs(sparse_meta_data, kwargs)

        if previous_mask.is_full_mask():
            return previous_mask

        tensor_dims: AttentionTensorDimensions = self._extract_tensor_dimensions(keys, queries)
        effective_heavy_size: int = self._calculate_effective_heavy_size(
            tensor_dims.seq_len_keys
        )

        if self._should_use_full_attention(tensor_dims, effective_heavy_size):
            return self._create_full_mask(tensor_dims, previous_mask.dtype)

        remaining_kwargs: Dict[str, Any] = {k: v for k, v in kwargs.items() if k != "layer_idx"}
        hash_mask: Mask = self._create_hash_topk_mask(
            tensor_dims,
            effective_heavy_size,
            keys,
            queries,
            sparse_meta_data,
            previous_mask,
            layer_idx,
            **remaining_kwargs,
        )
        return previous_mask.merge_mask(hash_mask, inplace=False)
```

```python
        """Validate required inputs for hash attention computation and return layer_idx."""
        if sparse_meta_data is None:
            raise ValueError("sparse_meta_data cannot be None")

        layer_idx: Optional[int] = kwargs.get("layer_idx")
        if layer_idx is None:
            raise ValueError("layer_idx must be provided in kwargs")

        return layer_idx

->

        """Validate required inputs for hash attention computation and return layer_idx."""
        if sparse_meta_data is None:
            raise ValueError("sparse_meta_data cannot be None")

        layer_idx: Optional[int] = kwargs.get("layer_idx")
        if layer_idx is None:
            raise ValueError("layer_idx must be provided in kwargs")

        return layer_idx
```

```python
        """Extract weight matrices and bias vectors for a specific tensor type (key or query)."""
        weights: Dict[str, List[torch.Tensor]] = self.hat_weights[layer_idx]
        matrix_list: List[torch.Tensor] = weights[f"{tensor_type}_matrix"]
        bias_list: List[torch.Tensor] = weights[f"{tensor_type}_bias"]
        return matrix_list, bias_list

->

        """Extract weight matrices and bias vectors for a specific tensor type (key or query)."""
        weights: Dict[str, List[torch.Tensor]] = self.hat_weights[layer_idx]
        matrix_list: List[torch.Tensor] = weights[f"{tensor_type}_matrix"]
        bias_list: List[torch.Tensor] = weights[f"{tensor_type}_bias"]
        return matrix_list, bias_list
```

```python
        """Compute signatures for a specific tensor type (key or query)."""
        matrix_list, bias_list = self._extract_weights_for_tensor_type(
            layer_idx, tensor_type
        )
        return self._get_signatures(tensor, matrix_list, bias_list)

->

        """Compute signatures for a specific tensor type (key or query)."""
        matrix_list: List[torch.Tensor]
        bias_list: List[torch.Tensor]
        matrix_list, bias_list = self._extract_weights_for_tensor_type(
            layer_idx, tensor_type
        )
        return self._get_signatures(tensor, matrix_list, bias_list)
```

```python
        """Create hash attention top-K mask using hash-based scoring."""
        scores = self._compute_hashattention_score(
            queries, keys, sparse_meta_data, layer_idx, **kwargs
        )
        top_k_indices = self._get_topk_indices_from_inactive_positions(
            scores, previous_mask, heavy_size
        )
        return self._create_mask_from_rowise_indices(
            dims, top_k_indices, keys.device, previous_mask.dtype
        )

->

        """Create hash attention top-K mask using hash-based scoring."""
        scores: torch.Tensor = self._compute_hashattention_score(
            queries, keys, sparse_meta_data, layer_idx, **kwargs
        )
        top_k_indices: torch.Tensor = self._get_topk_indices_from_inactive_positions(
            scores, previous_mask, heavy_size
        )
        return self._create_mask_from_rowise_indices(
            dims, top_k_indices, keys.device, previous_mask.dtype
        )
```

```python
        """Compute signatures using MLP layers with activation and sign function."""
        activation_map: Dict[str, Callable[[torch.Tensor], torch.Tensor]] = {
            "relu": torch.nn.functional.relu,
            "silu": torch.nn.functional.silu,
            "gelu": torch.nn.functional.gelu,
            "tanh": torch.nn.functional.tanh,
        }

        if self.hat_mlp_activation not in activation_map:
            raise ValueError(
                f"Unsupported activation function: {self.hat_mlp_activation}"
            )

        activation_fn: Callable[[torch.Tensor], torch.Tensor] = activation_map[self.hat_mlp_activation]
        signatures: torch.Tensor = input_tensor

->

        """Compute signatures using MLP layers with activation and sign function."""
        activation_map: Dict[str, Callable[[torch.Tensor], torch.Tensor]] = {
            "relu": torch.nn.functional.relu,
            "silu": torch.nn.functional.silu,
            "gelu": torch.nn.functional.gelu,
            "tanh": torch.nn.functional.tanh,
        }

        if self.hat_mlp_activation not in activation_map:
            raise ValueError(
                f"Unsupported activation function: {self.hat_mlp_activation}"
            )

        activation_fn: Callable[[torch.Tensor], torch.Tensor] = activation_map[self.hat_mlp_activation]
        signatures: torch.Tensor = input_tensor
```

```python
        for i in range(len(matrix_list) - 1):
            signatures = self._apply_single_layer(
                signatures, matrix_list[i], bias_list[i]
            )
            signatures = activation_fn(signatures)

        if len(matrix_list) > 0:
            signatures = self._apply_single_layer(
                signatures, matrix_list[-1], bias_list[-1]
            )

->

        for i in range(len(matrix_list) - 1):
            signatures = self._apply_single_layer(
                signatures, matrix_list[i], bias_list[i]
            )
            signatures = activation_fn(signatures)

        if len(matrix_list) > 0:
            signatures = self._apply_single_layer(
                signatures, matrix_list[-1], bias_list[-1]
            )
```

```python
        """Apply a single linear layer (matrix multiplication + bias addition)."""
        # (B,H,s,d) x (H,d,d_out) -> (B,H,s,d_out)
        output = torch.einsum("bhsd,hde->bhse", input_tensor, weight_matrix)
        # (B,H,s,d_out) + (H,d_out) -> (B,H,s,d_out)
        output = output + bias_vector.unsqueeze(0).unsqueeze(2)
        return output

->

        """Apply a single linear layer (matrix multiplication + bias addition)."""
        # (B,H,s,d) x (H,d,d_out) -> (B,H,s,d_out)
        output: torch.Tensor = torch.einsum("bhsd,hde->bhse", input_tensor, weight_matrix)
        # (B,H,s,d_out) + (H,d_out) -> (B,H,s,d_out)
        output = output + bias_vector.unsqueeze(0).unsqueeze(2)
        return output
```

```python
        """Update key signatures in sparse_meta_data and return concatenated signatures."""
        self._initialize_key_signature_cache(sparse_meta_data, layer_idx)
        cached_signatures, new_keys = self._determine_new_keys(
            keys, sparse_meta_data, layer_idx
        )

        if new_keys is None:
            assert (
                cached_signatures is not None
            ), "cached_signatures should not be None when new_keys is None"
            return cached_signatures

        new_signatures = self._compute_signatures_for_tensor_type(
            new_keys, layer_idx, "key"
        )
        return self._update_and_return_key_signatures(
            cached_signatures, new_signatures, sparse_meta_data, layer_idx
        )

->

        """Update key signatures in sparse_meta_data and return concatenated signatures."""
        self._initialize_key_signature_cache(sparse_meta_data, layer_idx)
        cached_signatures: Optional[torch.Tensor]
        new_keys: Optional[torch.Tensor]
        cached_signatures, new_keys = self._determine_new_keys(
            keys, sparse_meta_data, layer_idx
        )

        if new_keys is None:
            assert (
                cached_signatures is not None
            ), "cached_signatures should not be None when new_keys is None"
            return cached_signatures

        new_signatures: torch.Tensor = self._compute_signatures_for_tensor_type(
            new_keys, layer_idx, "key"
        )
        return self._update_and_return_key_signatures(
            cached_signatures, new_signatures, sparse_meta_data, layer_idx
        )
```

```python
        """
        cached_signatures = sparse_meta_data["key"][layer_idx]

        if cached_signatures is None:
            return None, keys

        cached_num_keys = cached_signatures.shape[2]
        current_num_keys = keys.shape[2]

        if current_num_keys < cached_num_keys:
            raise ValueError(
                f"Current number of keys ({current_num_keys}) is less than cached number of keys ({cached_num_keys})"
            )
        elif current_num_keys > cached_num_keys:
            new_keys = keys[:, :, cached_num_keys:, :]
            return cached_signatures, new_keys
        else:
            return cached_signatures, None

->

        """
        cached_signatures: Optional[torch.Tensor] = sparse_meta_data["key"][layer_idx]

        if cached_signatures is None:
            return None, keys

        cached_num_keys: int = cached_signatures.shape[2]
        current_num_keys: int = keys.shape[2]

        if current_num_keys < cached_num_keys:
            raise ValueError(
                f"Current number of keys ({current_num_keys}) is less than cached number of keys ({cached_num_keys})"
            )
        elif current_num_keys > cached_num_keys:
            new_keys: torch.Tensor = keys[:, :, cached_num_keys:, :]
            return cached_signatures, new_keys
        else:
            return cached_signatures, None
```

```python
        """Update the cache with new signatures and return the complete signature tensor."""
        if cached_signatures is None:
            sparse_meta_data["key"][layer_idx] = new_signatures
            return new_signatures
        else:
            concatenated_signatures = torch.cat(
                [cached_signatures, new_signatures], dim=2
            )
            sparse_meta_data["key"][layer_idx] = concatenated_signatures
            return concatenated_signatures

->

        """Update the cache with new signatures and return the complete signature tensor."""
        if cached_signatures is None:
            sparse_meta_data["key"][layer_idx] = new_signatures
            return new_signatures
        else:
            concatenated_signatures: torch.Tensor = torch.cat(
                [cached_signatures, new_signatures], dim=2
            )
            sparse_meta_data["key"][layer_idx] = concatenated_signatures
            return concatenated_signatures
```

```python
        """Compute hash attention scores using query and key signatures."""
        num_key_value_groups = _get_num_key_value_groups(queries, keys)
        processed_keys = repeat_kv(keys, num_key_value_groups)

        key_signatures = self._update_key_signatures(
            processed_keys, sparse_meta_data, layer_idx, **kwargs
        )
        query_signatures = self._compute_signatures_for_tensor_type(
            queries, layer_idx, "query"
        )

        # (B, H, #queries, hat_bits) x (B, H, hat_bits, #keys) -> (B, H, #queries, #keys)
        return torch.matmul(query_signatures, key_signatures.transpose(-2, -1))

->

        """Compute hash attention scores using query and key signatures."""
        num_key_value_groups: int = _get_num_key_value_groups(queries, keys)
        processed_keys: torch.Tensor = repeat_kv(keys, num_key_value_groups)

        key_signatures: torch.Tensor = self._update_key_signatures(
            processed_keys, sparse_meta_data, layer_idx, **kwargs
        )
        query_signatures: torch.Tensor = self._compute_signatures_for_tensor_type(
            queries, layer_idx, "query"
        )

        # (B, H, #queries, hat_bits) x (B, H, hat_bits, #keys) -> (B, H, #queries, #keys)
        return torch.matmul(query_signatures, key_signatures.transpose(-2, -1))
```

Perfect! I have successfully completed the task of adding type annotations to all statements of the form `a = f(b)` throughout the sparse_attention folder. Here's a summary of what was accomplished:

## Summary

I systematically went through all Python files in the sparse_attention folder and added type annotations to statements of the form `a = f(b)`. The changes were made to the following files:

### Core Files:
- **kv_utils.py** - Added type annotations to tensor operations
- **mask_attention_utils.py** - Added type annotations to attention computation functions  
- **mask.py** - Added type annotations to mask operations and tensor manipulations
- **hugging_face.py** - Added type annotations to HuggingFace integration functions

### Attention Mechanism Files:
- **research_attention/base.py** - Added type annotations to research attention methods
- **efficient_attention/base.py** - Added type annotations to efficient attention methods
- **double_sparsity.py** - Verified no statements needed annotations (pass statements only)
- **hash_attention.py** - Verified no statements needed annotations (pass statements only)

### Masker Files:
- **maskers/base.py** - Added type annotations to masker base classes
- **fixed/base.py** - Verified existing annotations were correct
- **basic_fixed.py** - Added type annotations to fixed pattern maskers
- **oracle_top_k.py** - Added type annotations to oracle top-K masker
- **hashattention_top_k.py** - Added type annotations to hash attention top-K masker

## Key Changes Made:
- Added type annotations like `tensor: torch.Tensor = function_call()`
- Added type annotations like `size: int = calculate_size()`
- Added type annotations like `mask: Mask = create_mask()`
- Properly handled tuple unpacking with separate type annotations
- Imported necessary types like `Optional`, `Tuple`, etc.

The codebase now has proper type annotations for all statements of the form `a = f(b)` throughout the sparse_attention folder, which will improve code clarity, help with static analysis, and make the code more maintainable.

