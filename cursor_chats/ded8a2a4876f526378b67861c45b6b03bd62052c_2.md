# Example usage of MicroMetricLogger
_Exported on 7/21/2025 at 15:45:09 PDT from Cursor (1.2.4)_

---

**User**

what is an example usage of MicroMetricLogger


---

**Cursor**

I'll examine the `MicroMetricLogger` code to understand its usage and provide you with examples.


Based on the code I've examined, I can provide you with comprehensive examples of how to use the `MicroMetricLogger`. The logger is a singleton pattern implementation for logging micro metrics with queue-based architecture.

## Basic Usage Example

Here's a simple example of how to use the `MicroMetricLogger`:

```python
from sparse_attention_hub.metric_logging import MicroMetricLogger

# Initialize the logger (singleton pattern)
logger = MicroMetricLogger(
    log_path="./experiment_logs",
    flush_every=50,
    metrics_to_log=["density", "accuracy", "latency"]
)

# Register metrics with their data types
logger.register_metric("density", float)
logger.register_metric("accuracy", float)
logger.register_metric("latency", float)

# Log metrics with optional metadata
logger.log("density", 0.15, metadata={"layer": 1, "head": 0})
logger.log("accuracy", 0.85, metadata={"benchmark": "longbench", "model": "gpt2"})
logger.log("latency", 0.002, metadata={"device": "gpu", "batch_size": 32})

# Force flush to disk
logger.flush()
```

## Advanced Usage Example

Here's a more comprehensive example showing different configuration options:

```python
from sparse_attention_hub.metric_logging import MicroMetricLogger
import time

# Initialize with "all" metrics enabled
logger = MicroMetricLogger(
    log_path="./detailed_logs",
    flush_every=100,  # Flush every 100 events
    flush_interval=60.0,  # Or flush every 60 seconds
    metrics_to_log="all"  # Enable all registered metrics
)

# Register various metric types
logger.register_metric("attention_density", float)
logger.register_metric("computation_time", float)
logger.register_metric("memory_usage", int)
logger.register_metric("sparsity_ratio", float)
logger.register_metric("model_accuracy", float)

# Simulate logging during model training/inference
for epoch in range(3):
    for batch in range(10):
        # Simulate some computation
        start_time = time.time()
        
        # Log various metrics
        logger.log("attention_density", 0.12, {
            "epoch": epoch,
            "batch": batch,
            "layer": 0
        })
        
        logger.log("computation_time", time.time() - start_time, {
            "epoch": epoch,
            "batch": batch,
            "device": "gpu"
        })
        
        logger.log("memory_usage", 2048, {
            "epoch": epoch,
            "batch": batch,
            "device": "gpu"
        })
        
        logger.log("sparsity_ratio", 0.88, {
            "epoch": epoch,
            "batch": batch,
            "attention_type": "sparse"
        })
        
        logger.log("model_accuracy", 0.92, {
            "epoch": epoch,
            "batch": batch,
            "metric_type": "validation"
        })

# Final flush
logger.flush()
```

## Key Features Demonstrated

1. **Singleton Pattern**: Only one logger instance exists across your application
2. **Queue-based Logging**: Events are buffered in memory and flushed periodically
3. **Auto-inferred Location**: The logger automatically detects where the log call came from
4. **Flexible Metadata**: You can attach any dictionary of metadata to each log entry
5. **Selective Logging**: You can enable/disable specific metrics
6. **JSONL Output**: Logs are written in JSONL format for easy parsing

## Output Format

The logger creates files like `metrics_20241201.jsonl` with entries like:

```json
{"timestamp": "2024-12-01T10:30:00.123456", "metric_enum": "density", "value": 0.15, "metadata": {"layer": 1, "head": 0}, "location": "your_module.your_function"}
{"timestamp": "2024-12-01T10:30:00.124567", "metric_enum": "accuracy", "value": 0.85, "metadata": {"benchmark": "longbench", "model": "gpt2"}, "location": "your_module.your_class.your_method"}
```

## Configuration Options

- `log_path`: Directory where log files are stored
- `flush_every`: Number of events before auto-flush
- `flush_interval`: Time interval (seconds) before auto-flush
- `metrics_to_log`: List of metric identifiers to enable, or "all" for all registered metrics

The logger is designed to be lightweight and efficient, making it suitable for high-frequency logging during model training or inference.

```python
from sparse_attention_hub.metric_logging import MicroMetricLogger

# Initialize the logger (singleton pattern)
logger = MicroMetricLogger(
    log_path="./experiment_logs",
    flush_every=50,
    metrics_to_log=["density", "accuracy", "latency"]
)

# Register metrics with their data types
logger.register_metric("density", float)
logger.register_metric("accuracy", float)
logger.register_metric("latency", float)

# Log metrics with optional metadata
logger.log("density", 0.15, metadata={"layer": 1, "head": 0})
logger.log("accuracy", 0.85, metadata={"benchmark": "longbench", "model": "gpt2"})
logger.log("latency", 0.002, metadata={"device": "gpu", "batch_size": 32})

# Force flush to disk
logger.flush()
```

```python
from sparse_attention_hub.metric_logging import MicroMetricLogger
import time

# Initialize with "all" metrics enabled
logger = MicroMetricLogger(
    log_path="./detailed_logs",
    flush_every=100,  # Flush every 100 events
    flush_interval=60.0,  # Or flush every 60 seconds
    metrics_to_log="all"  # Enable all registered metrics
)

# Register various metric types
logger.register_metric("attention_density", float)
logger.register_metric("computation_time", float)
logger.register_metric("memory_usage", int)
logger.register_metric("sparsity_ratio", float)
logger.register_metric("model_accuracy", float)

# Simulate logging during model training/inference
for epoch in range(3):
    for batch in range(10):
        # Simulate some computation
        start_time = time.time()
        
        # Log various metrics
        logger.log("attention_density", 0.12, {
            "epoch": epoch,
            "batch": batch,
            "layer": 0
        })
        
        logger.log("computation_time", time.time() - start_time, {
            "epoch": epoch,
            "batch": batch,
            "device": "gpu"
        })
        
        logger.log("memory_usage", 2048, {
            "epoch": epoch,
            "batch": batch,
            "device": "gpu"
        })
        
        logger.log("sparsity_ratio", 0.88, {
            "epoch": epoch,
            "batch": batch,
            "attention_type": "sparse"
        })
        
        logger.log("model_accuracy", 0.92, {
            "epoch": epoch,
            "batch": batch,
            "metric_type": "validation"
        })

# Final flush
logger.flush()
```

```json
{"timestamp": "2024-12-01T10:30:00.123456", "metric_enum": "density", "value": 0.15, "metadata": {"layer": 1, "head": 0}, "location": "your_module.your_function"}
{"timestamp": "2024-12-01T10:30:00.124567", "metric_enum": "accuracy", "value": 0.85, "metadata": {"benchmark": "longbench", "model": "gpt2"}, "location": "your_module.your_class.your_method"}
```

---

**User**

Since, MicroMetricLogger can be accesssed anywhere in the code, i think we need to separate out the 

registering, enabling and logging

1. registering should work even when log_path, flush parameters and metrics_to_log is not defined. .. 
 -- so registering should happen at a class level. 

2. enabling can be done via a simple enable function which sets the list. by default it should be set to empty

3. MicroMetricLogger().log(...) should work only after log_path is defiend


---

**Cursor**

I understand your requirements. You want to separate the concerns so that:

1. **Registration** works at class level without needing configuration
2. **Enabling** is a simple operation that sets the list (empty by default)
3. **Logging** only works after `log_path` is defined

Let me examine the current implementation and then modify it according to your specifications.


Now I'll modify the `MicroMetricLogger` to implement your requirements. The key changes will be:

1. **Class-level registration**: Move registration to work without initialization
2. **Simple enabling**: Default to empty list, simple enable function
3. **Logging only after log_path**: Check if log_path is defined before allowing logging


```python
"""MicroMetricLogger implementation for sparse attention hub."""

import json
import os
import time
from collections import deque
from dataclasses import dataclass, asdict
from datetime import datetime
from typing import Any, Dict, List, Optional, Union
import inspect


@dataclass
class LogEvent:
    """Log event data structure."""
    timestamp: datetime
    metric_enum: str  # Metric identifier string
    value: Union[None, Any]
    metadata: Dict[str, Any]  # Additional context (layer, head, etc.)
    location: str  # Auto-inferred: "module.function" or "class.method"


class MicroMetricLogger:
    """Singleton logger for micro metrics with queue-based architecture."""

    _instance: Optional["MicroMetricLogger"] = None
    _initialized: bool = False
    
    # Class-level storage for registered metrics (works without initialization)
    _registered_metrics: Dict[str, type] = {}  # identifier -> dtype mapping

    def __new__(cls, *args, **kwargs) -> "MicroMetricLogger":
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self, 
                 log_path: Optional[str] = None,
                 flush_every: int = 100,  # Flush every N events
                 flush_interval: float = 60.0,  # Flush every N seconds
                 enabled_metrics: Union[List[str], str] = None):  # List of string identifiers to enable, or "all"
        if not self._initialized:
            self.log_path = log_path
            self.flush_every = flush_every
            self.flush_interval = flush_interval
            
            # Internal state
            self.log_queue: deque = deque(maxlen=10000)  # Circular buffer
            self.enabled_metrics: set = set()
            self.last_flush_time = time.time()
            
            # Enable metrics if log_path is provided
            if self.log_path is not None:
                self._ensure_log_directory()
                self.enable_metrics(enabled_metrics)
            
            MicroMetricLogger._initialized = True

    def _ensure_log_directory(self) -> None:
        """Ensure the log directory exists."""
        if self.log_path is not None:
            os.makedirs(self.log_path, exist_ok=True)

    def _get_calling_location(self) -> str:
        """Get the calling location using inspect module."""
        try:
            # Get the calling frame (skip this method and the log method)
            caller_frame = inspect.currentframe().f_back.f_back
            if caller_frame is None:
                return "unknown"
            
            # Get module name
            module = inspect.getmodule(caller_frame)
            module_name = module.__name__ if module else "unknown"
            
            # Get function/class name
            function_name = caller_frame.f_code.co_name
            
            # Try to get class name if it's a method
            class_name = None
            if 'self' in caller_frame.f_locals:
                class_name = caller_frame.f_locals['self'].__class__.__name__
            
            if class_name:
                return f"{module_name}.{class_name}.{function_name}"
            else:
                return f"{module_name}.{function_name}"
        except Exception:
            return "unknown"

    @classmethod
    def register_metric(cls, identifier: str, dtype: type) -> None:
        """Register a metric with its string identifier and expected data type.
        
        This works at class level and doesn't require initialization.
        """
        if identifier in cls._registered_metrics:
            print(f"Warning: Metric '{identifier}' is being re-registered")
        cls._registered_metrics[identifier] = dtype

    def enable_metrics(self, metrics: Union[List[str], str] = None) -> None:
        """Enable logging for specific metrics.
        
        Args:
            metrics: List of metric identifiers to enable, or "all" for all registered metrics.
                    If None, enables no metrics (empty list).
        """
        if metrics == "all":
            self.enabled_metrics = set(self._registered_metrics.keys())
        elif isinstance(metrics, (list, set)):
            # Only enable metrics that are registered
            valid_metrics = set(metrics) & set(self._registered_metrics.keys())
            invalid_metrics = set(metrics) - set(self._registered_metrics.keys())
            if invalid_metrics:
                print(f"Warning: Attempting to enable unregistered metrics: {invalid_metrics}")
            self.enabled_metrics = valid_metrics
        else:
            # Default to empty set
            self.enabled_metrics = set()

    def enable_metric(self, identifier: str) -> None:
        """Enable logging for a specific metric by string identifier."""
        if identifier in self._registered_metrics:
            self.enabled_metrics.add(identifier)
        else:
            print(f"Warning: Attempting to enable unregistered metric: {identifier}")

    def disable_metric(self, identifier: str) -> None:
        """Disable logging for a specific metric by string identifier."""
        self.enabled_metrics.discard(identifier)

    def log(self, identifier: str, value: Any, metadata: Dict[str, Any] = None) -> None:
        """Log a metric value with optional metadata. Location is auto-inferred.
        
        This only works if log_path is defined.
        """
        # Check if logging is configured
        if self.log_path is None:
            print(f"Warning: Cannot log metric '{identifier}' - log_path not defined. Use configure_logging() first.")
            return
            
        # Check if metric is enabled
        if identifier not in self.enabled_metrics:
            print(f"Warning: Attempting to log metric '{identifier}' which is not enabled")
            return
        
        # Create log event
        event = LogEvent(
            timestamp=datetime.now(),
            metric_enum=identifier,
            value=value,
            metadata=metadata or {},
            location=self._get_calling_location()
        )
        
        # Add to queue
        self.log_queue.append(event)
        
        # Check if we should flush
        if len(self.log_queue) >= self.flush_every:
            self.flush()

    def configure_logging(self, log_path: str, enabled_metrics: Union[List[str], str] = None) -> None:
        """Configure logging with a log path and optionally enable metrics.
        
        This must be called before logging can work.
        """
        self.log_path = log_path
        self._ensure_log_directory()
        self.enable_metrics(enabled_metrics)

    def flush(self) -> None:
        """Force flush the current queue to disk."""
        if not self.log_queue or self.log_path is None:
            return
            
        # Get current timestamp for filename
        timestamp = datetime.now().strftime("%Y%m%d")
        filename = f"metrics_{timestamp}.jsonl"
        filepath = os.path.join(self.log_path, filename)
        
        # Write events to file
        with open(filepath, "a", encoding="utf-8") as f:
            while self.log_queue:
                event = self.log_queue.popleft()
                # Convert dataclass to dict and serialize
                event_dict = asdict(event)
                # Convert datetime to ISO format string
                event_dict["timestamp"] = event_dict["timestamp"].isoformat()
                f.write(json.dumps(event_dict) + "\n")
        
        self.last_flush_time = time.time()

    def request_metric(self, identifier: str) -> bool:
        """Check if a specific metric is requested for logging."""
        return identifier in self.enabled_metrics

    @classmethod
    def get_registered_metrics(cls) -> Dict[str, type]:
        """Get all registered metrics at class level."""
        return cls._registered_metrics.copy()

    def get_enabled_metrics(self) -> set:
        """Get currently enabled metrics."""
        return self.enabled_metrics.copy()

    def is_logging_configured(self) -> bool:
        """Check if logging is configured (log_path is set)."""
        return self.log_path is not None

    def __del__(self):
        """Cleanup when logger is destroyed."""
        self.flush()  # Final flush
```

Perfect! Now let me create a simple example to demonstrate the new usage pattern:


```python
#!/usr/bin/env python3
"""Example usage of the modified MicroMetricLogger."""

from sparse_attention_hub.metric_logging import MicroMetricLogger

def example_usage():
    """Demonstrate the new usage pattern."""
    
    print("=== MicroMetricLogger Example Usage ===\n")
    
    # 1. REGISTRATION (works at class level, no initialization needed)
    print("1. Registering metrics (class level):")
    MicroMetricLogger.register_metric("density", float)
    MicroMetricLogger.register_metric("accuracy", float)
    MicroMetricLogger.register_metric("latency", float)
    MicroMetricLogger.register_metric("memory_usage", int)
    
    print(f"   Registered metrics: {list(MicroMetricLogger.get_registered_metrics().keys())}")
    
    # 2. INITIALIZATION (optional - can be done later)
    print("\n2. Initializing logger (optional):")
    logger = MicroMetricLogger()  # No log_path yet, so no logging enabled
    
    print(f"   Logging configured: {logger.is_logging_configured()}")
    print(f"   Enabled metrics: {list(logger.get_enabled_metrics())}")
    
    # 3. ENABLING METRICS (simple operation, empty by default)
    print("\n3. Enabling metrics:")
    logger.enable_metrics(["density", "accuracy"])  # Enable specific metrics
    print(f"   Enabled metrics: {list(logger.get_enabled_metrics())}")
    
    # Try to log without log_path (should fail gracefully)
    print("\n4. Attempting to log without log_path:")
    logger.log("density", 0.15, {"layer": 1})  # Should show warning
    
    # 5. CONFIGURE LOGGING (required for actual logging)
    print("\n5. Configuring logging:")
    logger.configure_logging("./example_logs", enabled_metrics="all")
    print(f"   Logging configured: {logger.is_logging_configured()}")
    print(f"   Enabled metrics: {list(logger.get_enabled_metrics())}")
    
    # 6. ACTUAL LOGGING (now works)
    print("\n6. Logging metrics:")
    logger.log("density", 0.15, {"layer": 1, "head": 0})
    logger.log("accuracy", 0.85, {"benchmark": "test", "model": "gpt2"})
    logger.log("latency", 0.002, {"device": "gpu", "batch_size": 32})
    logger.log("memory_usage", 1024, {"device": "gpu"})
    
    # Force flush to see the output
    logger.flush()
    print("   Logs written to ./example_logs/")
    
    # 7. ALTERNATIVE USAGE PATTERN
    print("\n7. Alternative usage pattern:")
    
    # Register more metrics
    MicroMetricLogger.register_metric("sparsity_ratio", float)
    MicroMetricLogger.register_metric("computation_time", float)
    
    # Create new logger instance (same singleton)
    logger2 = MicroMetricLogger()
    
    # Configure and enable in one step
    logger2.configure_logging("./alt_logs", enabled_metrics=["sparsity_ratio", "computation_time"])
    
    # Log
    logger2.log("sparsity_ratio", 0.88, {"attention_type": "sparse"})
    logger2.log("computation_time", 0.001, {"operation": "forward_pass"})
    
    logger2.flush()
    print("   Alternative logs written to ./alt_logs/")
    
    print("\n=== Example Complete ===")

if __name__ == "__main__":
    example_usage()
```

Now let's test the example to make sure it works:


There's an import issue with the main package. Let me create a simpler test that directly imports the logger:


```python
#!/usr/bin/env python3
"""Direct test of the modified MicroMetricLogger."""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'sparse_attention_hub'))

from metric_logging.logger import MicroMetricLogger

def test_logger():
    """Test the new logger functionality."""
    
    print("=== Testing Modified MicroMetricLogger ===\n")
    
    # 1. REGISTRATION (works at class level, no initialization needed)
    print("1. Registering metrics (class level):")
    MicroMetricLogger.register_metric("density", float)
    MicroMetricLogger.register_metric("accuracy", float)
    MicroMetricLogger.register_metric("latency", float)
    
    print(f"   Registered metrics: {list(MicroMetricLogger.get_registered_metrics().keys())}")
    
    # 2. INITIALIZATION (optional - can be done later)
    print("\n2. Initializing logger (optional):")
    logger = MicroMetricLogger()  # No log_path yet, so no logging enabled
    
    print(f"   Logging configured: {logger.is_logging_configured()}")
    print(f"   Enabled metrics: {list(logger.get_enabled_metrics())}")
    
    # 3. ENABLING METRICS (simple operation, empty by default)
    print("\n3. Enabling metrics:")
    logger.enable_metrics(["density", "accuracy"])  # Enable specific metrics
    print(f"   Enabled metrics: {list(logger.get_enabled_metrics())}")
    
    # Try to log without log_path (should fail gracefully)
    print("\n4. Attempting to log without log_path:")
    logger.log("density", 0.15, {"layer": 1})  # Should show warning
    
    # 5. CONFIGURE LOGGING (required for actual logging)
    print("\n5. Configuring logging:")
    logger.configure_logging("./test_logs", enabled_metrics="all")
    print(f"   Logging configured: {logger.is_logging_configured()}")
    print(f"   Enabled metrics: {list(logger.get_enabled_metrics())}")
    
    # 6. ACTUAL LOGGING (now works)
    print("\n6. Logging metrics:")
    logger.log("density", 0.15, {"layer": 1, "head": 0})
    logger.log("accuracy", 0.85, {"benchmark": "test", "model": "gpt2"})
    logger.log("latency", 0.002, {"device": "gpu", "batch_size": 32})
    
    # Force flush to see the output
    logger.flush()
    print("   Logs written to ./test_logs/")
    
    # 7. TEST SINGLETON BEHAVIOR
    print("\n7. Testing singleton behavior:")
    logger2 = MicroMetricLogger()
    print(f"   Same instance: {logger is logger2}")
    print(f"   Logging configured: {logger2.is_logging_configured()}")
    print(f"   Enabled metrics: {list(logger2.get_enabled_metrics())}")
    
    # 8. TEST CLASS-LEVEL REGISTRATION
    print("\n8. Testing class-level registration:")
    MicroMetricLogger.register_metric("new_metric", str)
    print(f"   All registered metrics: {list(MicroMetricLogger.get_registered_metrics().keys())}")
    
    print("\n=== Test Complete ===")

if __name__ == "__main__":
    test_logger()
```

Perfect! Let's check the generated log file to make sure it's working correctly:


Excellent! The implementation is working perfectly. Let me clean up the test files and provide a summary:


