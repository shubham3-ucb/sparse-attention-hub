# Update mask attention utility function
_Exported on 7/8/2025 at 01:11:25 PDT from Cursor (1.2.2)_

---

**User**

In mask_attention_utils.py, in _compute_masked_exp_attention_weights, use apply_inv_mask instead of apply_mask . Make appropriate changesin tests

---

**Cursor**

I'll help you modify the `_compute_masked_exp_attention_weights` function to use `apply_inv_mask` instead of `apply_mask` and update the related tests. Let me first examine the Mask class to understand the difference between these methods.

Now I understand the difference between `apply_mask` and `apply_inv_mask`:

- `apply_mask`: multiplies the input tensor by the mask values (element-wise)
- `apply_inv_mask`: applies the inverse mask, dividing by the mask values (multiplying by 1/mask_value where mask_value != 0)

Let me update the `_compute_masked_exp_attention_weights` function and then find the related tests:

```python
    if not sparse_attention_mask.is_empty():
        exp_attention_weights = sparse_attention_mask.apply_mask(exp_attention_weights)

->

    if not sparse_attention_mask.is_empty():
        exp_attention_weights = sparse_attention_mask.apply_inv_mask(exp_attention_weights)
```

Now let me search for tests that need to be updated for this change:

Now I need to update the test that checks the sparse mask behavior. The key test that needs updating is `test_compute_masked_attention_weights_with_sparse_mask` since it tests the specific behavior of applying the sparse mask.

Now I need to update the tests that check the sparse mask behavior. The key change is that with `apply_inv_mask`, the expected result should be `expected_exp_weights * (1.0 / dense_mask)` instead of `expected_exp_weights * dense_mask`.

```python
        # verify that the non-zero values are what we expect
        expected_raw_weights = torch.matmul(queries, keys.transpose(-2, -1)) * scaling
        expected_max = torch.max(expected_raw_weights, dim=-1, keepdim=True)[0]
        expected_exp_weights = torch.exp(expected_raw_weights - expected_max)
        assert torch.allclose(
            result[unmasked_positions],
            expected_exp_weights[unmasked_positions] * dense_mask[unmasked_positions],
        )

->

        # verify that the non-zero values are what we expect
        expected_raw_weights = torch.matmul(queries, keys.transpose(-2, -1)) * scaling
        expected_max = torch.max(expected_raw_weights, dim=-1, keepdim=True)[0]
        expected_exp_weights = torch.exp(expected_raw_weights - expected_max)
        assert torch.allclose(
            result[unmasked_positions],
            expected_exp_weights[unmasked_positions] * (1.0 / dense_mask[unmasked_positions]),
        )
```

Now I need to update the second test that also checks the sparse mask behavior:

```python
        # verify that the non-zero values are what we expect
        expected_raw_weights = torch.matmul(queries, keys.transpose(-2, -1)) * scaling

        expected_max = torch.max(
            expected_raw_weights + attention_mask, dim=-1, keepdim=True
        )[0]
        expected_exp_weights = torch.exp(expected_raw_weights - expected_max)
        assert torch.allclose(
            result[active_positions],
            expected_exp_weights[active_positions] * dense_mask[active_positions],
        )

->

        # verify that the non-zero values are what we expect
        expected_raw_weights = torch.matmul(queries, keys.transpose(-2, -1)) * scaling

        expected_max = torch.max(
            expected_raw_weights + attention_mask, dim=-1, keepdim=True
        )[0]
        expected_exp_weights = torch.exp(expected_raw_weights - expected_max)
        assert torch.allclose(
            result[active_positions],
            expected_exp_weights[active_positions] * (1.0 / dense_mask[active_positions]),
        )
```

Now let's run the tests to verify that our changes work correctly:

