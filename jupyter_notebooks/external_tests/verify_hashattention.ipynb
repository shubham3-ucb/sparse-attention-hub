{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# HashAttention Implementation Comparison\n",
        "\n",
        "This notebook compares two implementations of hashattention:\n",
        "1. USA module from `hashattention_llama.py`\n",
        "2. HashAttentionTopKMasker from the sparse attention framework\n",
        "\n",
        "The goal is to verify that both implementations produce equivalent results for the signature computation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Install Hashattention-1.0\n",
        "! pip install git+https://github.com/xAlg-ai/Hashattention-1.0.git"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setup and Imports\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.5.0\n",
            "Device: Tesla V100-SXM2-32GB-LS\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('..')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from typing import Dict, List, Any\n",
        "from dataclasses import dataclass\n",
        "import math\n",
        "from einops import rearrange\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {torch.cuda.get_device_name() if torch.cuda.is_available() else 'CPU'}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Import HashAttention Components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CENTER 0\n"
          ]
        }
      ],
      "source": [
        "# Import USA module components\n",
        "from hashattention.hashattention_llama import USA, DEFAULT_USA_CFG, SignSTE, ste_sign\n",
        "\n",
        "# Import HashAttentionTopKMasker components\n",
        "from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed.implementations.hashattention_top_k import (\n",
        "    HashAttentionTopKMasker,\n",
        "    HashAttentionTopKMaskerConfig\n",
        ")\n",
        "from sparse_attention_hub.sparse_attention.utils.mask import Mask\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Configuration and Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration:\n",
            "  num_heads: 4\n",
            "  head_dim: 64\n",
            "  batch_size: 2\n",
            "  seq_len_q: 8\n",
            "  seq_len_k: 12\n",
            "  USA config: {'lth_int_dim': 32, 'lth_final_dim': 16, 'lth_thold': 0.0, 'lth_num_layers': 2}\n"
          ]
        }
      ],
      "source": [
        "# Configuration parameters\n",
        "num_heads = 4\n",
        "head_dim = 64\n",
        "batch_size = 2\n",
        "seq_len_q = 8\n",
        "seq_len_k = 12\n",
        "layer_idx = 0\n",
        "\n",
        "# USA configuration\n",
        "usa_config = {\n",
        "    'lth_int_dim': 32,\n",
        "    'lth_final_dim': 16,  # This will be our hat_bits\n",
        "    'lth_thold': 0.0,\n",
        "    'lth_num_layers': 2\n",
        "}\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  num_heads: {num_heads}\")\n",
        "print(f\"  head_dim: {head_dim}\")\n",
        "print(f\"  batch_size: {batch_size}\")\n",
        "print(f\"  seq_len_q: {seq_len_q}\")\n",
        "print(f\"  seq_len_k: {seq_len_k}\")\n",
        "print(f\"  USA config: {usa_config}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Create USA Module\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "USA module created with 2 layers\n",
            "Key transformation heads: 4\n",
            "Query transformation heads: 4\n",
            "\n",
            "First head key transformation:\n",
            "  Layer 0: Linear(in_features=64, out_features=32, bias=True)\n",
            "  Layer 1: SiLU()\n",
            "  Layer 2: Linear(in_features=32, out_features=16, bias=True)\n",
            "\n",
            "First head query transformation:\n",
            "  Layer 0: Linear(in_features=64, out_features=32, bias=True)\n",
            "  Layer 1: SiLU()\n",
            "  Layer 2: Linear(in_features=32, out_features=16, bias=True)\n"
          ]
        }
      ],
      "source": [
        "# Create USA module\n",
        "usa_module = USA(num_heads=num_heads, head_dim=head_dim, usa_params=usa_config)\n",
        "usa_module.eval()\n",
        "\n",
        "print(f\"USA module created with {usa_config['lth_num_layers']} layers\")\n",
        "print(f\"Key transformation heads: {len(usa_module.learning_to_hash_transformation_k)}\")\n",
        "print(f\"Query transformation heads: {len(usa_module.learning_to_hash_transformation_q)}\")\n",
        "\n",
        "# Print architecture for first head\n",
        "print(f\"\\nFirst head key transformation:\")\n",
        "for i, layer in enumerate(usa_module.learning_to_hash_transformation_k[0]):\n",
        "    print(f\"  Layer {i}: {layer}\")\n",
        "    \n",
        "print(f\"\\nFirst head query transformation:\")\n",
        "for i, layer in enumerate(usa_module.learning_to_hash_transformation_q[0]):\n",
        "    print(f\"  Layer {i}: {layer}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Extract Weights from USA Module\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2 linear layers in key transformation\n",
            "Found 2 linear layers in query transformation\n",
            "Key layer 0: weight shape torch.Size([4, 64, 32]), bias shape torch.Size([4, 32])\n",
            "Key layer 1: weight shape torch.Size([4, 32, 16]), bias shape torch.Size([4, 16])\n",
            "Query layer 0: weight shape torch.Size([4, 64, 32]), bias shape torch.Size([4, 32])\n",
            "Query layer 1: weight shape torch.Size([4, 32, 16]), bias shape torch.Size([4, 16])\n",
            "\n",
            "Extracted weights structure:\n",
            "  Key matrices: 2 layers\n",
            "  Query matrices: 2 layers\n"
          ]
        }
      ],
      "source": [
        "def extract_weights_from_usa(usa_module, layer_idx=0):\n",
        "    \"\"\"\n",
        "    Extract weights from USA module and format them for HashAttentionTopKMasker.\n",
        "    \n",
        "    Args:\n",
        "        usa_module: USA module instance\n",
        "        layer_idx: Layer index for the weights dictionary\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary formatted for HashAttentionTopKMaskerConfig.hat_weights\n",
        "    \"\"\"\n",
        "    num_heads = usa_module.num_heads\n",
        "    \n",
        "    # Initialize weight structure\n",
        "    hat_weights = {\n",
        "        layer_idx: {\n",
        "            \"key_matrix\": [],\n",
        "            \"key_bias\": [],\n",
        "            \"query_matrix\": [],\n",
        "            \"query_bias\": []\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Extract weights for each MLP layer\n",
        "    # We need to stack weights across heads to get (H, d_in, d_out) tensors\n",
        "    \n",
        "    # Get the number of linear layers (excluding activations)\n",
        "    k_linear_layers = [layer for layer in usa_module.learning_to_hash_transformation_k[0] if isinstance(layer, nn.Linear)]\n",
        "    q_linear_layers = [layer for layer in usa_module.learning_to_hash_transformation_q[0] if isinstance(layer, nn.Linear)]\n",
        "    \n",
        "    print(f\"Found {len(k_linear_layers)} linear layers in key transformation\")\n",
        "    print(f\"Found {len(q_linear_layers)} linear layers in query transformation\")\n",
        "    \n",
        "    # Extract key matrices and biases\n",
        "    for layer_idx_mlp in range(len(k_linear_layers)):\n",
        "        # Stack weights and biases across heads\n",
        "        key_weights = []\n",
        "        key_biases = []\n",
        "        \n",
        "        for head_idx in range(num_heads):\n",
        "            # Get the actual linear layer from the sequential module\n",
        "            actual_layer = None\n",
        "            layer_count = 0\n",
        "            for module in usa_module.learning_to_hash_transformation_k[head_idx]:\n",
        "                if isinstance(module, nn.Linear):\n",
        "                    if layer_count == layer_idx_mlp:\n",
        "                        actual_layer = module\n",
        "                        break\n",
        "                    layer_count += 1\n",
        "            \n",
        "            if actual_layer is not None:\n",
        "                key_weights.append(actual_layer.weight.detach().clone().T)  # Transpose for correct shape\n",
        "                key_biases.append(actual_layer.bias.detach().clone())\n",
        "        \n",
        "        # Stack to get (H, d_in, d_out) and (H, d_out) shapes\n",
        "        key_matrix = torch.stack(key_weights, dim=0)\n",
        "        key_bias = torch.stack(key_biases, dim=0)\n",
        "        \n",
        "        hat_weights[layer_idx][\"key_matrix\"].append(key_matrix)\n",
        "        hat_weights[layer_idx][\"key_bias\"].append(key_bias)\n",
        "        \n",
        "        print(f\"Key layer {layer_idx_mlp}: weight shape {key_matrix.shape}, bias shape {key_bias.shape}\")\n",
        "    \n",
        "    # Extract query matrices and biases\n",
        "    for layer_idx_mlp in range(len(q_linear_layers)):\n",
        "        # Stack weights and biases across heads\n",
        "        query_weights = []\n",
        "        query_biases = []\n",
        "        \n",
        "        for head_idx in range(num_heads):\n",
        "            # Get the actual linear layer from the sequential module\n",
        "            actual_layer = None\n",
        "            layer_count = 0\n",
        "            for module in usa_module.learning_to_hash_transformation_q[head_idx]:\n",
        "                if isinstance(module, nn.Linear):\n",
        "                    if layer_count == layer_idx_mlp:\n",
        "                        actual_layer = module\n",
        "                        break\n",
        "                    layer_count += 1\n",
        "            \n",
        "            if actual_layer is not None:\n",
        "                query_weights.append(actual_layer.weight.detach().clone().T)  # Transpose for correct shape\n",
        "                query_biases.append(actual_layer.bias.detach().clone())\n",
        "        \n",
        "        # Stack to get (H, d_in, d_out) and (H, d_out) shapes\n",
        "        query_matrix = torch.stack(query_weights, dim=0)\n",
        "        query_bias = torch.stack(query_biases, dim=0)\n",
        "        \n",
        "        hat_weights[layer_idx][\"query_matrix\"].append(query_matrix)\n",
        "        hat_weights[layer_idx][\"query_bias\"].append(query_bias)\n",
        "        \n",
        "        print(f\"Query layer {layer_idx_mlp}: weight shape {query_matrix.shape}, bias shape {query_bias.shape}\")\n",
        "    \n",
        "    return hat_weights\n",
        "\n",
        "# Extract weights\n",
        "hat_weights = extract_weights_from_usa(usa_module, layer_idx=0)\n",
        "print(f\"\\nExtracted weights structure:\")\n",
        "print(f\"  Key matrices: {len(hat_weights[0]['key_matrix'])} layers\")\n",
        "print(f\"  Query matrices: {len(hat_weights[0]['query_matrix'])} layers\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Create Sample Inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample inputs created:\n",
            "  Keys shape: torch.Size([2, 4, 12, 64])\n",
            "  Queries shape: torch.Size([2, 4, 8, 64])\n",
            "  Values shape: torch.Size([2, 4, 12, 64])\n",
            "  Previous mask shape: (2, 4, 8, 12)\n",
            "  Previous mask is full: False\n"
          ]
        }
      ],
      "source": [
        "# Create sample inputs\n",
        "torch.manual_seed(42)  # For reproducibility\n",
        "\n",
        "# Create sample key and query tensors\n",
        "# Shape: (batch_size, num_heads, seq_len, head_dim)\n",
        "sample_keys = torch.randn(batch_size, num_heads, seq_len_k, head_dim, dtype=torch.float32)\n",
        "sample_queries = torch.randn(batch_size, num_heads, seq_len_q, head_dim, dtype=torch.float32)\n",
        "sample_values = torch.randn(batch_size, num_heads, seq_len_k, head_dim, dtype=torch.float32)\n",
        "\n",
        "print(f\"Sample inputs created:\")\n",
        "print(f\"  Keys shape: {sample_keys.shape}\")\n",
        "print(f\"  Queries shape: {sample_queries.shape}\")\n",
        "print(f\"  Values shape: {sample_values.shape}\")\n",
        "\n",
        "# Create attention mask (optional)\n",
        "attention_mask = None\n",
        "\n",
        "# Create previous mask for the masker\n",
        "mask_shape = (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "previous_mask = Mask.create_empty_mask(mask_shape)\n",
        "\n",
        "print(f\"  Previous mask shape: {mask_shape}\")\n",
        "print(f\"  Previous mask is full: {previous_mask.is_full_mask()}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test USA Module Forward Pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== USA Module Forward Pass ===\n",
            "USA scores shape: torch.Size([2, 4, 8, 12])\n",
            "USA key embeddings shape: torch.Size([2, 4, 12, 16])\n",
            "USA key signatures shape: torch.Size([2, 4, 12, 16])\n",
            "USA query signatures shape: torch.Size([2, 4, 8, 16])\n",
            "USA key signatures range: [-1.000, 1.000]\n",
            "USA query signatures range: [-1.000, 1.000]\n",
            "Manual USA scores shape: torch.Size([2, 4, 8, 12])\n",
            "Manual USA scores range: [-12.000, 12.000]\n"
          ]
        }
      ],
      "source": [
        "# Test USA module forward pass\n",
        "print(\"=== USA Module Forward Pass ===\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    # USA module expects (B, H, seq_len, head_dim) input\n",
        "    usa_scores, usa_key_embeddings = usa_module(sample_keys, sample_queries, hard=True)\n",
        "    \n",
        "    print(f\"USA scores shape: {usa_scores.shape}\")\n",
        "    print(f\"USA key embeddings shape: {usa_key_embeddings.shape}\")\n",
        "    \n",
        "    # Also test individual embeddings\n",
        "    usa_key_sigs = usa_module.k_embedding(sample_keys, hard=True)\n",
        "    usa_query_sigs = usa_module.q_embedding(sample_queries, hard=True)\n",
        "    \n",
        "    print(f\"USA key signatures shape: {usa_key_sigs.shape}\")\n",
        "    print(f\"USA query signatures shape: {usa_query_sigs.shape}\")\n",
        "    \n",
        "    # Check value range (should be -1 or 1 for hard=True)\n",
        "    print(f\"USA key signatures range: [{usa_key_sigs.min():.3f}, {usa_key_sigs.max():.3f}]\")\n",
        "    print(f\"USA query signatures range: [{usa_query_sigs.min():.3f}, {usa_query_sigs.max():.3f}]\")\n",
        "    \n",
        "    # Compute manual scores for comparison\n",
        "    usa_manual_scores = torch.matmul(usa_query_sigs, usa_key_sigs.transpose(-2, -1))\n",
        "    print(f\"Manual USA scores shape: {usa_manual_scores.shape}\")\n",
        "    print(f\"Manual USA scores range: [{usa_manual_scores.min():.3f}, {usa_manual_scores.max():.3f}]\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Create HashAttentionTopKMasker\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HashAttentionTopKMasker created with:\n",
            "  heavy_size: 0.5\n",
            "  hat_bits: 16\n",
            "  hat_mlp_layers: 2\n",
            "  hat_mlp_hidden_size: 32\n",
            "  hat_mlp_activation: silu\n"
          ]
        }
      ],
      "source": [
        "# Create HashAttentionTopKMaskerConfig\n",
        "masker_config = HashAttentionTopKMaskerConfig(\n",
        "    heavy_size=0.5,  # Use 50% of keys\n",
        "    hat_bits=usa_config['lth_final_dim'],\n",
        "    hat_mlp_layers=usa_config['lth_num_layers'],\n",
        "    hat_mlp_hidden_size=usa_config['lth_int_dim'],\n",
        "    hat_mlp_activation=\"silu\",  # USA uses SiLU activation\n",
        "    hat_weights=hat_weights\n",
        ")\n",
        "\n",
        "# Create HashAttentionTopKMasker\n",
        "masker = HashAttentionTopKMasker(masker_config)\n",
        "\n",
        "print(f\"HashAttentionTopKMasker created with:\")\n",
        "print(f\"  heavy_size: {masker_config.heavy_size}\")\n",
        "print(f\"  hat_bits: {masker_config.hat_bits}\")\n",
        "print(f\"  hat_mlp_layers: {masker_config.hat_mlp_layers}\")\n",
        "print(f\"  hat_mlp_hidden_size: {masker_config.hat_mlp_hidden_size}\")\n",
        "print(f\"  hat_mlp_activation: {masker_config.hat_mlp_activation}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test HashAttentionTopKMasker _get_signatures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== HashAttentionTopKMasker _get_signatures ===\n",
            "Masker key signatures shape: torch.Size([2, 4, 12, 16])\n",
            "Masker key signatures range: [-1.000, 1.000]\n",
            "Masker query signatures shape: torch.Size([2, 4, 8, 16])\n",
            "Masker query signatures range: [-1.000, 1.000]\n",
            "Manual masker scores shape: torch.Size([2, 4, 8, 12])\n",
            "Manual masker scores range: [-12.000, 12.000]\n"
          ]
        }
      ],
      "source": [
        "# Test HashAttentionTopKMasker _get_signatures\n",
        "print(\"=== HashAttentionTopKMasker _get_signatures ===\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Test key signatures\n",
        "    key_matrix_list = hat_weights[0][\"key_matrix\"]\n",
        "    key_bias_list = hat_weights[0][\"key_bias\"]\n",
        "    \n",
        "    masker_key_sigs = masker._get_signatures(\n",
        "        sample_keys, key_matrix_list, key_bias_list\n",
        "    )\n",
        "    \n",
        "    print(f\"Masker key signatures shape: {masker_key_sigs.shape}\")\n",
        "    print(f\"Masker key signatures range: [{masker_key_sigs.min():.3f}, {masker_key_sigs.max():.3f}]\")\n",
        "    \n",
        "    # Test query signatures\n",
        "    query_matrix_list = hat_weights[0][\"query_matrix\"]\n",
        "    query_bias_list = hat_weights[0][\"query_bias\"]\n",
        "    \n",
        "    masker_query_sigs = masker._get_signatures(\n",
        "        sample_queries, query_matrix_list, query_bias_list\n",
        "    )\n",
        "    \n",
        "    print(f\"Masker query signatures shape: {masker_query_sigs.shape}\")\n",
        "    print(f\"Masker query signatures range: [{masker_query_sigs.min():.3f}, {masker_query_sigs.max():.3f}]\")\n",
        "    \n",
        "    # Compute manual scores for comparison\n",
        "    masker_manual_scores = torch.matmul(masker_query_sigs, masker_key_sigs.transpose(-2, -1))\n",
        "    print(f\"Manual masker scores shape: {masker_manual_scores.shape}\")\n",
        "    print(f\"Manual masker scores range: [{masker_manual_scores.min():.3f}, {masker_manual_scores.max():.3f}]\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Compare Signatures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Signature Comparison ===\n",
            "Key signatures:\n",
            "  Max absolute difference: 0.000000\n",
            "  Mean absolute difference: 0.000000\n",
            "  Are they close? True\n",
            "\n",
            "Query signatures:\n",
            "  Max absolute difference: 0.000000\n",
            "  Mean absolute difference: 0.000000\n",
            "  Are they close? True\n",
            "\n",
            "Manual scores:\n",
            "  Max absolute difference: 0.000000\n",
            "  Mean absolute difference: 0.000000\n",
            "  Are they close? True\n"
          ]
        }
      ],
      "source": [
        "# Compare the signatures\n",
        "print(\"=== Signature Comparison ===\")\n",
        "\n",
        "# Compare key signatures\n",
        "key_diff = torch.abs(usa_key_sigs - masker_key_sigs)\n",
        "print(f\"Key signatures:\")\n",
        "print(f\"  Max absolute difference: {key_diff.max():.6f}\")\n",
        "print(f\"  Mean absolute difference: {key_diff.mean():.6f}\")\n",
        "print(f\"  Are they close? {torch.allclose(usa_key_sigs, masker_key_sigs, atol=1e-6)}\")\n",
        "\n",
        "# Compare query signatures\n",
        "query_diff = torch.abs(usa_query_sigs - masker_query_sigs)\n",
        "print(f\"\\nQuery signatures:\")\n",
        "print(f\"  Max absolute difference: {query_diff.max():.6f}\")\n",
        "print(f\"  Mean absolute difference: {query_diff.mean():.6f}\")\n",
        "print(f\"  Are they close? {torch.allclose(usa_query_sigs, masker_query_sigs, atol=1e-6)}\")\n",
        "\n",
        "# Compare manual scores\n",
        "scores_diff = torch.abs(usa_manual_scores - masker_manual_scores)\n",
        "print(f\"\\nManual scores:\")\n",
        "print(f\"  Max absolute difference: {scores_diff.max():.6f}\")\n",
        "print(f\"  Mean absolute difference: {scores_diff.mean():.6f}\")\n",
        "print(f\"  Are they close? {torch.allclose(usa_manual_scores, masker_manual_scores, atol=1e-6)}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary and Comparison Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== SUMMARY ===\n",
            "\n",
            "1. USA Module:\n",
            "   - Successfully created with 2 layers\n",
            "   - Produces signatures of shape: torch.Size([2, 4, 12, 16])\n",
            "   - Signature values in range: [-1.000, 1.000]\n",
            "\n",
            "2. HashAttentionTopKMasker:\n",
            "   - Successfully created with weights extracted from USA module\n",
            "   - Produces signatures of shape: torch.Size([2, 4, 12, 16])\n",
            "   - Signature values in range: [-1.000, 1.000]\n",
            "\n",
            "3. Comparison Results:\n",
            "   - Key signatures match: True\n",
            "   - Query signatures match: True\n",
            "   - Manual scores match: True\n",
            "   - Max key signature difference: 0.000000\n",
            "   - Max query signature difference: 0.000000\n",
            "\n",
            "✅ SUCCESS: Both implementations produce identical results!\n"
          ]
        }
      ],
      "source": [
        "# Summary of results\n",
        "print(\"=== SUMMARY ===\")\n",
        "print()\n",
        "print(\"1. USA Module:\")\n",
        "print(f\"   - Successfully created with {usa_config['lth_num_layers']} layers\")\n",
        "print(f\"   - Produces signatures of shape: {usa_key_sigs.shape}\")\n",
        "print(f\"   - Signature values in range: [{usa_key_sigs.min():.3f}, {usa_key_sigs.max():.3f}]\")\n",
        "print()\n",
        "print(\"2. HashAttentionTopKMasker:\")\n",
        "print(f\"   - Successfully created with weights extracted from USA module\")\n",
        "print(f\"   - Produces signatures of shape: {masker_key_sigs.shape}\")\n",
        "print(f\"   - Signature values in range: [{masker_key_sigs.min():.3f}, {masker_key_sigs.max():.3f}]\")\n",
        "print()\n",
        "print(\"3. Comparison Results:\")\n",
        "print(f\"   - Key signatures match: {torch.allclose(usa_key_sigs, masker_key_sigs, atol=1e-6)}\")\n",
        "print(f\"   - Query signatures match: {torch.allclose(usa_query_sigs, masker_query_sigs, atol=1e-6)}\")\n",
        "print(f\"   - Manual scores match: {torch.allclose(usa_manual_scores, masker_manual_scores, atol=1e-6)}\")\n",
        "print(f\"   - Max key signature difference: {key_diff.max():.6f}\")\n",
        "print(f\"   - Max query signature difference: {query_diff.max():.6f}\")\n",
        "print()\n",
        "if torch.allclose(usa_key_sigs, masker_key_sigs, atol=1e-6) and torch.allclose(usa_query_sigs, masker_query_sigs, atol=1e-6):\n",
        "    print(\"✅ SUCCESS: Both implementations produce identical results!\")\n",
        "else:\n",
        "    print(\"❌ MISMATCH: Implementations produce different results.\")\n",
        "    print(\"   This might be due to:\")\n",
        "    print(\"   - Different weight extraction/formatting\")\n",
        "    print(\"   - Different activation functions\")\n",
        "    print(\"   - Different numerical precision\")\n",
        "    print(\"   - Different tensor operations\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "infllm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
